---
layout: single
title: "머신러닝 프로세스 개괄"
date: "2022-01-27 18:27:30 +0900"
last_modified_at: "2022-01-27 18:27:30 +0900"
---

# 머신러닝 프로세스 개괄

# 분석 준비 -> 모델평가 및 결정

1. Data set 분할 

- 학습 데이터를 랜덤으로 학습/ 검증 셋 (train/validation) 분할

- 테스트 셋(test)도 준비

- 학습 데이터: 70 ~ 90% / 검증 데이터: 10 ~ 20% / 테스트데이터: 10 ~ 20%

2. 데이터 전처리

- 데이터의 표준화/정규화

> > Standardization (표준화)
: 특성들의 평균을 0, 분산을 1 로 스케일링하는 것입니다.
즉, 특성들을 정규분포로 만드는 것입니다.

> > Normalization (정규화)
: 특성들을 특정 범위(주로 [0,1]) 로 스케일링 하는 것입니다.
가작 작은 값은 0, 가장 큰 값은 1 로 변환되므로, 모든 특성들은 [0, 1] 범위를 갖게됩니다.



- 범주형 데이터를 one-hot-encoding (더미변수)로 만들어 주기(0,1로 이루어져야함)

- 특성변수의 축약

- 표준화(평균 0/ 표준편차 1) 또는 Min-max 정규화

- 고차원의 경우 PCA 방법 등으로 차원 축소

- 단위의 조정:scaling


> > 스케일링이란?
머신러닝을 위한 데이터셋을 정제할 때, 특성별로 데이터의 스케일이 다르다면 어떤 일이 벌어질까요?
> > >예를 들어,
X1은 0 부터 1 사이의 값을 갖고 , 
X2 는 1000000 부터 1000000000000 사이의 값을 갖고 , 
y 는 1000000 부터 100000000 사이의 값을 갖는다고 가정한다면
X1 특성은 y 를 예측하는데 큰 영향을 주지 않는 것으로 생각할 수 있습니다.
때문에 특성별로 데이터의 스케일이 다르다면, 머신러닝이 잘 동작하지 않을 수 있습니다.
따라서, 데이터 스케일링 작업을 통해, 모든 특성의 범위(또는 분포)를 같게 만들어줘야합니다.

3. 모델 적용

- 머신러닝 알고리즘 적용 

- 모델 평가 

- 예측/분류/비지도 알고리즘에 데이터 학습

- 학습된 모델에 검증 데이터로 평가

- 정확도 및 과소/과대추정 여부 판단

- 파라미터 조정을 통한 최적 모델 결정 

4. Hyper Parameter 탐색 및 결정

- 다양한 하이퍼 파라미터 적용

- 최적의 하이퍼 파라미터 및 모델 결정 

- 최종 모델을 테스트 셋에 대해 성능을 평가

- 테스트 셋에 대한 정확도를 현재 데이터로 학습한 알고리즘 성능으로 제시

- 하이퍼 파라미터를 조정하며 데이터에 적합한 최적의 알고리즘을 찾는게 중요함
