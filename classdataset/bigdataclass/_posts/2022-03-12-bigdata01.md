---
layout: single
title: "Why Most Published Research Findings Are False 과제 요약 및 정리 01"
date: "2022-03-12 22:20:22 +0900"
last_modified_at: "2022-03-12 22:20:22 +0900"
---

# Why Most Published Research Findings Are False 과제 요약 및 정리 01

- 출처 :[https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124)

## Abstract

### Summary

There is increasing concern that most current published research findings are false. 

The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. 

In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. 

Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. 

Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. 

In this essay, I discuss the implications of these problems for the conduct and interpretation of research.

현재의 공개 된 연구 결과가 거짓이라는 우려가 증가하고 있습니다. 

연구 주장이 사실이라는 확률은 편견, 같은 질문에 대한 다른 연구들의 수, 중요하게는 각 과학 분야에서 조사된 관계 간의 관계가 없는 것에 대한 비율에 결정 될 수 있습니다.

이 프레임 워크에서, 분야에서 수행 된 연구들이 작을 때, 효과 크기가 작을 때, 테스트 된 관계의 더 많은 수와 적은 사전 선택 사항이 있을 때, 설계, 정의, 결과 및 분석 모드에서 더 큰 유연성이 있는 경우, 더 큰 재정 및 기타 관심과 편견이 있을 때, 그리고 더 많은 팀이 통계적 유의성을 찾아내어 과학 분야에 관여 할 때, 연구 결과가 진실 일 가능성이 적습니다. 

시뮬레이션은 대부분의 연구 설계 및 환경에서 연구 주장이 진실보다 거짓이라는 것으로 나타났습니다.

또한 많은 현재의 과학 분야의 경우, 주장되는 연구 결과가 종종 일반적인 편견의 정확한 측정들이 될 수 있습니다. 

이 글에서는 연구의 행위와 해석을 위해 이러한 문제의 시사점들을 논의합니다.

Published research findings are sometimes refuted by subsequent evidence, with ensuing confusion and disappointment.

Refutation and controversy is seen across the range of research designs, from clinical trials and traditional epidemiological studies [1–3] to the most modern molecular research [4,5]. 

There is increasing concern that in modern research, false findings may be the majority or even the vast majority of published research claims [6–8]. 

However, this should not be surprising. 

It can be proven that most claimed research findings are false. 

Here I will examine the key factors that influence this problem and some corollaries thereof.

공개 된 연구 결과는 혼란과 실망으로 후속 증거로 인해 때로는 상반됩니다.

논박 및 논쟁은 임상 시험 및 전통적인 역학 연구 [1-3]에서 가장 현대적인 분자 연구에 이르기까지 연구 설계들의 범위에 걸쳐 볼 수 있습니다 [4,5].

현대 연구에서 우려되는 점이 많아지고 있다. 거짓 발견은 대다수 또는 심지어 대다수가 공개 된 연구 주장(6-8)이 될 수 있다.

그러나 이것은 놀라운 일은 아닙니다.

대부분의 연구 결과가 거짓이라는 것을 입증 할 수 있습니다.

여기에서는 이 문제와 그 일부 환상에 영향을 미치는 핵심 요소를 검토 할 것입니다.



## Modeling the Framework for False Positive Findings

Research is not most appropriately represented and summarized by p-values, but, unfortunately, there is a widespread notion that medical research articles should be interpreted based only on p-values. 

Research findings are defined here as any relationship reaching formal statistical significance, e.g., effective interventions, risk factors, or associations. 

“Negative” research is also very useful. “Negative” is actually a misnomer, and the misinterpretation is widespread. 

However, here we will target relationships that investigators claim exist, rather than null findings.


연구는 p 값에 의해 가장 적절하게 대표적이고 요약되지는 않지만, 불행히도 의료 연구 기사가 p 값에만 오로지 해석되어야한다는 광범위한 개념이 있습니다. 

연구 결과는 공식 통계적 유의성, 효과적인 해결책, 위험 요소 또는 연관에 의한 모든 관계로서 정의됩니다.

"부정하는"연구도 매우 유용합니다. "부정하는"은 실제로 부적절한 단어이며, 오해는 널리 퍼져 있습니다.

그러나 여기서 우리는 아무 가치 없는 발견이 아닌 존재하는 관계를 표적으로하는 것을 목표로 할 것입니다.



> It can be proven that most claimed research findings are false

As has been shown previously, the probability that a research finding is indeed true depends on the prior probability of it being true (before doing the study), the statistical power of the study, and the level of statistical significance [10,11]. 

Consider a 2 × 2 table in which research findings are compared against the gold standard of true relationships in a scientific field.

In a research field both true and false hypotheses can be made about the presence of relationships. 

Let R be the ratio of the number of “true relationships” to “no relationships” among those tested in the field. 

R is characteristic of the field and can vary a lot depending on whether the field targets highly likely relationships or searches for only one or a few true relationships among thousands and millions of hypotheses that may be postulated. 

> 가장 주장 된 연구 결과가 거짓임을 입증 할 수 있습니다.

이전에 설명한 바와 같이, 연구 결과가 확실히 정확할 확률은 정확한 사전 확률 (연구를 하기 전에), 통계적 능력과 통계적 유의성의 수준에 달려 있습니다 [10,11].

연구 결과가 과학 분야에서 정확한 관계의 황금 기준과 비교되는 2 × 2 표를 고려하십시오.

연구 분야에서는 관계가 존재하는 것에 대해 진실과 거짓 가설을 모두 만들 수 있습니다.

분야에서 테스트 한 것들 중에서 "관계가 없음"과의 "참인 관계"의 수의 비율을 R로 하자.

* r은 분야의 특징이며, 수천과 수백만 가지 가설 중 하나 또는 몇 가지 참인 관계를 겪을 가능성이 높거나 몇 가지 참인 관계를 탐색할지 여부에 따라 많이 다를 수 있습니다.


Let us also consider, for computational simplicity, circumscribed fields where either there is only one true relationship (among many that can be hypothesized) or the power is similar to find any of the several existing true relationships.

The pre-study probability of a relationship being true is R/(R + 1). 


고려하자면, 계산 단순성에서 , 참인 관계 (가설이 될 수 있는 많은 것) 또는 기존의 참의 관계 중 하나를 찾는 것과 유사합니다.

참의 관계의 사전 연구 확률은 R / (R + 1)입니다.



The probability of a study finding a true relationship reflects the power 1 - β (one minus the Type II error rate). 

The probability of claiming a relationship when none truly exists reflects the Type I error rate, α. 

Assuming that c relationships are being probed in the field, the expected values of the 2 × 2 table are given in Table 1.

After a research finding has been claimed based on achieving formal statistical significance, the post-study probability that it is true is the positive predictive value, PPV. 


참인 관계를 찾는 연구 확률은  1 - β (Type II Type 오차율 1 마이너스)를 반영합니다. 

존재하지 않을 때 관계를 주장 할 확률은 Type I의 오차율, α를 반영합니다.

C 관계가 분야에 시험되고 있다고 가정하면, 2 × 2 테이블의 예상 값은 표 1에 주어집니다. 

공식 통계적 유의성을 달성하는 것에 대한 연구 결과가 주장 된 후에는 참인 사전 연구 확률이 긍정적인 예측 값인 PPV 값이다.  


The PPV is also the complementary probability of what Wacholder et al. have called the false positive report probability [10]. 

According to the 2 × 2 table, one gets PPV = (1 - β)R/(R - βR + α). 

A research finding is thus more likely true than false if (1 - β)R > α. 

Since usually the vast majority of investigators depend on a = 0.05, this means that a research finding is more likely true than false if (1 - β)R > 0.05.


PPV는 또한 거짓 긍정적인 보고서 확률이라고 불리는 Wacholder et al을 보완 할 확률이기도합니다.[10].

2 × 2 표에 따르면, 하나는 PPV = (1 - β) R / (R-βR + α)을 얻는다.

따라서 (1 - β)R > α 이면 거짓보다는 연구 결과가 거의 참일 될 가능성이 큽니다.

일반적으로 대부분의 조사관의 대다수는 a = 0.05에 달려 있기 때문에 이것은 연구결과가 (1 - β)R> 0.05 이면 거짓보다 참이 될 가능성이 훨씬 크다는 것을 의미합니다.

> PPV

The positive predictive value (PPV), also called precision, is defined as




```python
from IPython.display import Image  # 주피터 노트북에 이미지 삽입
Image("C://Users/MyCom/jupyter-tutorial/대학원자료/빅데이터/data/20220312_163146_1.png")
```




![output_6_0]({{ site.gdrive_url_prefix }}1aeszDixP_87BmIoUYSQ89EQr67Lc2tac)

    



where a "true positive" is the event that the test makes a positive prediction, and the subject has a positive result under the gold standard, and a "false positive" is the event that the test makes a positive prediction, and the subject has a negative result under the gold standard. The ideal value of the PPV, with a perfect test, is 1 (100%), and the worst possible value would be zero.

In case-control studies the PPV has to be computed from sensitivity, specificity, but also including the prevalence:


```python
from IPython.display import Image  # 주피터 노트북에 이미지 삽입
Image("C://Users/MyCom/jupyter-tutorial/대학원자료/빅데이터/data/20220312_163146_2.png")
```




![output_8_0]({{ site.gdrive_url_prefix }}14FpqqsWbej9Y4xZzmBbDgVdoiWRAb6qP)

    




```python
from IPython.display import Image  # 주피터 노트북에 이미지 삽입
Image("C://Users/MyCom/jupyter-tutorial/대학원자료/빅데이터/data/20220312_163146_3.png")
```



![output_9_0]({{ site.gdrive_url_prefix }}1pd1gMItRnUhAYwIXk5yIlgkHAfZt0ma3)




> p-value(유의 확률, significance probability)

- P값이 0.05보다 크면 귀무가설이 참일 가능성이 높다. 

- p-value는, 귀무가설(null hypothesis, H0)이 맞다는 전제 하에, 통계값(statistics)이 실제로 관측된 값 이상일 확률을 의미한다.

  일반적으로 p-value는 어떤 가설을 전제로, 그 가설이 맞는다는 가정 하에, 내가 현재 구한 통계값이 얼마나 자주 나올 것인가, 를 의미한다고 할 수 있다. p-value는 가설검정이라는 것이 전체 데이터를 갖고 하는 것이 아닌 sampling 된 데이터를 갖고 하는 것이기 때문이다.

  일반적으로 p-value < 0.05 혹은 0.01을 기준으로 합니다. 계산된 p-value가 기준값보다 작은 경우 귀무가설을 기각하는 것으로 즉, 극단적으로 귀무가설이 일어날 확률이 매우 낮은 상태를 의미합니다.

- P값이 0.05 미만인 경우 평균 간에 차이가 없다는 귀무 가설을 기각하고 유의한 차이가 있다는 결론을 내리게 됩니다. P값이 0.05보다 크면 큰 차이가 존재한다는 결론을 내릴 수 없습니다.

- 0.05 미만이면 유의하고(의미가 있다) 0.05를 초과하면 유의하지 않습니다.

- (통계적 유의성은 모집단에 대한 가설이 가지는 통계적 의미를 말한다. 다시 말해서, 어떤 실험 결과 자료를 두고 “통계적으로 유의하다.”라고 하는 것은 확률적으로 봐서 단순한 우연이라고 생각되지 않을 정도로 의미가 있다는 뜻이다. 반대로 “통계적으로 유의하지 않다.”라고 하는 것은 실험 결과가 단순한 우연일 수도 있다는 뜻이다.)


### FPRP: False-positive report probabilit 

The function calculates the false positive report probability (FPRP), the probability of no true association beteween a genetic variant and disease given a statistically significant finding, which depends not only on the observed P value but also on both the prior probability that the assocition is real and the statistical power of the test. An associate result is the false negative reported probability (FNRP). See example for the recommended steps.

The FPRP and FNRP are derived as follows. Let H_0=null hypothesis (no association), H_A=alternative hypothesis (association). Since classic frequentist theory considers they are fixed, one has to resort to Bayesian framework by introduing prior, π=P(H_0=TRUE)=P(association). 

```html

Let T=test statistic, and P(T>z_α | H_0=TRUE)=P(rejecting\ H_0 | H_0=TRUE)=α, P(T>z_α|H_0=FALSE)=P(rejecting\ H_0 | H_A=TRUE)=1-β. The joint probability of test and truth of hypothesis can be expressed by α, β and π.

```

- 출처: [https://rdrr.io/cran/gap/man/FPRP.html](https://rdrr.io/cran/gap/man/FPRP.html)

What is less well appreciated is that bias and the extent of repeated independent testing by different teams of investigators around the globe may further distort this picture and may lead to even smaller probabilities of the research findings being indeed true. 

We will try to model these two factors in the context of similar 2 × 2 tables.

잘 알려지지 않은 것은 편차와 전세계의 조사관의 다른 팀의 반복적이고 독립적인 테스트의 정도는 이 그림을 더 왜곡 할 수 있으며 실제로 참인 연구 결과의 더 작은 확률로 이어질 수 있다는 것입니다.

우리는 유사한 2 × 2 테이블의 맥락에서 이 두 가지 요소를 모델링하려고 노력할 것입니다.

## Bias

First, let us define bias as the combination of various design, data, analysis, and presentation factors that tend to produce research findings when they should not be produced. 

Let u be the proportion of probed analyses that would not have been “research findings,” but nevertheless end up presented and reported as such, because of bias. 

Bias should not be confused with chance variability that causes some findings to be false by chance even though the study design, data, analysis, and presentation are perfect. 

첫째, 연구 결과를 생성 되지 않을때 연구 결과를 생산하는 경향이 있는 다양한 디자인, 데이터, 분석 및 프리젠 테이션 요인의 조합으로 편차를 정의하십시오.

"연구 결과"가 아니었지만 분석된 비율을 u라고 하자. 그럼에도 불구하고 결국은 편차로 인해 보도되지 않았습니다.

편차는 연구 설계, 데이터, 분석 및 프리젠 테이션이 완벽하더라도 기회에 따라 일부 결과가 거짓이라는 것을 일으키는 기회 변동성과 혼동되어서는 안됩니다.





Bias can entail manipulation in the analysis or reporting of findings. 

Selective or distorted reporting is a typical form of such bias. 

We may assume that u does not depend on whether a true relationship exists or not. 

This is not an unreasonable assumption, since typically it is impossible to know which relationships are indeed true. 

In the presence of bias (Table 2), one gets PPV = ([1 - β]R + uβR)/(R + α − βR + u − uα + uβR), and PPV decreases with increasing u, unless 1 − β ≤ α, i.e., 1 − β ≤ 0.05 for most situations. 

편차는 연구결과의 분석이나 보고에서 수정을 수반 할 수 있습니다.

선택적 또는 왜곡 된 보고는 이러한 편차의 전형적인 형태입니다.

우리는 U가 참인 관계가 존재하는지 여부에 따라 의존하지 않는다고 가정 할 수 있습니다.

이것은 일반적으로 어떤 관계가 참인지 아는 것이 불가능하기 때문에 불합리한 가정이 아닙니다.

편차 (표 2)의 존재하에, 하나는 PPV = ([1 - β] R + uβR) / (R + α - βR + U-Uα + UβR)을 가져오고, 1 - β ≤ 0.05 인 대부분의 상황이고 1 − β ≤ α가 제외하지 않는 한 u가 증가함에 따라 PPV가 감소합니다. 




Thus, with increasing bias, the chances that a research finding is true diminish considerably. 

This is shown for different levels of power and for different pre-study odds in Figure 1. Conversely, true research findings may occasionally be annulled because of reverse bias. 

For example, with large measurement errors relationships are lost in noise [12], or investigators use data inefficiently or fail to notice statistically significant relationships, or there may be conflicts of interest that tend to “bury” significant findings [13]. 

There is no good large-scale empirical evidence on how frequently such reverse bias may occur across diverse research fields. 

따라서 편차가 증가함에 따라 연구 결과가 참이 될 가능성이 상당히 감소합니다.

그림 1은 다른 수준의 검정량 및 다른 사전 학습 확률에 대한 것이다.

예를 들어, 노이즈 [12]에서 대규모 측정 오차 관계가 손실되거나 조사관이 비효율적으로 데이터를 사용하거나 통계적으로 중요한 관계를 알 수 없거나 중요한 결과를 숨기는 경향이 있는 이해 상충이 있을 수 있습니다 [13].

다양한 연구 분야에서 이러한 역방향 편차가 얼마나 자주 발생하는지에 대한 대규모 실증적인 증거가 없습니다.


However, it is probably fair to say that reverse bias is not as common. 

Moreover measurement errors and inefficient use of data are probably becoming less frequent problems, since measurement error has decreased with technological advances in the molecular era and investigators are becoming increasingly sophisticated about their data. 

Regardless, reverse bias may be modeled in the same way as bias above. 

Also reverse bias should not be confused with chance variability that may lead to missing a true relationship because of chance.

그러나 역방향 편차가 공통적이지 않다는 것을 말하는 것은 아마도 공평합니다.

또한 측정 오차와 비효율적인 데이터의 사용은 단순한 구조 시대의 기술적 진보와 수사관의 기술적 진보가 점점 더 정교해지고 있기 때문에 덜 빈번한 문제가 될 것이다.

역방향 편차는 위의 편차와 동일한 방식으로 모델링 될 수 있습니다.

또한 역뱡향 편차는 기회로 인해 참인 관계를 잃게 할 수 있는 기회 변동성와 혼동되어서는 안됩니다.



> BIAS (편차) 

- 편향이란, 예측값이 정답과 얼마나 다른가(차이가 있는가, 떨어져 있는가, 멀게 있는가 등등) 를 표현합니다.

- E[f^*(x)] : 예측값 들의 평균 입니다. 

- f(x): 정답값 입니다.

- 둘을 빼면 정답과 예측값이 서로 떨어진 거리를 알 수 있지요. 이 거리는, 정답과 예측값들이 서로 가까운지 먼 지를 알려주는 지표입니다.




```python
from IPython.display import Image  # 주피터 노트북에 이미지 삽입
Image("C://Users/MyCom/jupyter-tutorial/대학원자료/빅데이터/data/20220312_155212_1.png")
```




![output_14_0]({{ site.gdrive_url_prefix }}1I8F8VGZ67uGHyQu730y150DU1oGFdUmS)




## Testing by Several Independent Teams

Several independent teams may be addressing the same sets of research questions. 

As research efforts are globalized, it is practically the rule that several research teams, often dozens of them, may probe the same or similar questions.

Unfortunately, in some areas, the prevailing mentality until now has been to focus on isolated discoveries by single teams and interpret research experiments in isolation. 



여러 독립 팀이 동일한 연구 질문 집합을 해결할 수 있습니다. 

연구 노력이 세계화되어 있기 때문에, 실제로 여러 연구 팀의 규칙, 종종 수십 명이 동일하거나 유사한 질문을 탐구 할 수 있습니다.

불행히도, 일부 지역에서는 지금까지 지배적인 정신은 단일 팀의 분리 된 발견에 초점을 맞추고 연구 실험을 격리로 해석하는 것이었습니다.


The probability that at least one study, among several done on the same question, claims a statistically significant research finding is easy to estimate.

For n independent studies of equal power, the 2 × 2 table is shown in Table 3: PPV = R(1 − βn)/(R + 1 − [1 − α]n − Rβn) (not considering bias). 

With increasing number of independent studies, PPV tends to decrease, unless 1 - β < a, i.e., typically 1 − β < 0.05. 

This is shown for different levels of power and for different pre-study odds in Figure 2.

For n studies of different power, the term βn is replaced by the product of the terms βi for i = 1 to n, but inferences are similar.



동일한 질문에 대한 여러 가지 연구에서 적어도 하나의 연구가 통계적으로 중요한 연구 발견을 주장하는 확률은 추정하기 쉽습니다.


동일한 검정량에 대한 N 개의 독립적 인 연구를 위해 2 × 2 표는 표 3 : PPV = R(1 − βn)/(R + 1 − [1 − α]n − Rβn)  (편차를 고려하지 않음)에 표 3에 나타낸다.

독립적인 연구의 수가 증가함에 따라 PPV는 1 - β <a, 전형적으로 1 - β <0.05 인 경우를 제외하고는 감소하는 경향이있다.

이것은 다양한 수준의 검정량 및 그림 2의 다른 사전 학습 확률에 대해 표시됩니다.

N 개의 다른 검정량에 대한 연구를 위해, βn 용어는 i = 1에서 N의 용어 βi 용어로 대체되지만 추론은 유사합니다.

> 검정량

1. 검정 통계량 (Test Statistic)

  ㅇ 통계적 가설을 검정할 목적으로 사용되는 표본 통계량
     - 통계적 결론을 내릴 때 근거가 되는 통계량

  ㅇ 검정 통계량이, 귀무가설 기각역에 포함 여부에 따라,
     - 귀무가설의 수용(Accept),기각(Reject) 여부를 결정하게됨


2. 가설검정을 위한 주요 표본 통계량(검정 통계량)의 例) 

  ㅇ 모 평균의 검정을 위한 표본평균
  
  ㅇ 모 분산의 검정을 위한 표본분산
  
  ㅇ t 값 (t 검정 : 두 집단의 평균 간에 차이가 있는지를 비교 판단하는 검정)
  
  ㅇ z 값 (z 검정 : 모집단 평균과의 차이를 검정)
  
  ㅇ F 분포 검정통계량 등

## Corollaries

A practical example is shown in Box 1. Based on the above considerations, one may deduce several interesting corollaries about the probability that a research finding is indeed true.


### Box 1. An Example: Science at Low Pre-Study Odds

Let us assume that a team of investigators performs a whole genome association study to test whether any of 100,000 gene polymorphisms are associated with susceptibility to schizophrenia. 


Then R = 10/100,000 = 10−4, and the pre-study probability for any polymorphism to be associated with schizophrenia is also R/(R + 1) = 10−4. 

Let us also suppose that the study has 60% power to find an association with an odds ratio of 1.3 at α = 0.05. 

Then it can be estimated that if a statistically significant association is found with the p-value barely crossing the 0.05 threshold, the post-study probability that this is true increases about 12-fold compared with the pre-study probability, but it is still only 12 × 10−4.

수사관 팀이 100,000 유전자 다형성 중 어느 하나가 정신 분열증에 대한 민감성과 관련되어 있는지 여부를 테스트하기 위해 전체 게놈 연관 연구를 수행한다고 가정합시다. 

그런 다음 R = 10 / 100,000 = 10-4이고, 정신 분열증과 관련된 다형성에 대한 예비 연구 확률은 R / (R + 1) = 10-4이기도합니다. 

우리가 연구가 α = 0.05에서 1.3의 오즈비과의 연관성을 찾는 연구에 대해서도 60 %의 검정량을 가지고 있다고 가정 해 봅시다. 

그런 다음 통계적으로 유의 한 연관이 0.05 임계 값을 간신히 교차하는 p 값으로 발견되면, 이것이 참일 될 사전 연구 확률이 12 × 10-4 인 사전 연구 확률에 비해 약 12배 증가한다.
____________________________________________________________________


- Corollary 1: The smaller the studies conducted in a scientific field, the less likely the research findings are to be true. 

Small sample size means smaller power and, for all functions above, the PPV for a true research finding decreases as power decreases towards 1 − β = 0.05. 


corollary 1 : 과학 분야에서 수행 된 연구가 작을수록 연구 결과가 참일 가능성이 적습니다.

작은 샘플 크기는 더 작은 검정량을 의미하며, 위의 모든 기능에 대해 참인 연구 결과를 위한 PPV는 검정량이 1 - β = 0.05로 감소함에 따라 감소합니다.


- Corollary 2: The smaller the effect sizes in a scientific field, the less likely the research findings are to be true. 

Power is also related to the effect size. 

Thus research findings are more likely true in scientific fields with large effects, such as the impact of smoking on cancer or cardiovascular disease (relative risks 3–20), than in scientific fields where postulated effects are small, such as genetic risk factors for multigenetic diseases (relative risks 1.1–1.5) [7]. 

Modern epidemiology is increasingly obliged to target smaller effect sizes [16]. 

Consequently, the proportion of true research findings is expected to decrease. In the same line of thinking, if the true effect sizes are very small in a scientific field, this field is likely to be plagued by almost ubiquitous false positive claims. 

For example, if the majority of true genetic or nutritional determinants of complex diseases confer relative risks less than 1.05, genetic or nutritional epidemiology would be largely utopian endeavors.

검정량은 효과 크기와 관련이 있습니다. 

현대 역학은 점점 더 작은 효과 크기를 목표로하는 것입니다 [16]. 

결과적으로 참인 연구 결과의 비율은 감소 할 것으로 예상됩니다. 

동일한 사고방식 면에서 참인 효과 크기가 과학 분야에서 매우 작으면 이 분야는 거의 유비쿼터스 거짓 긍정적인 주장에 의해 성가시게 될 것입니다.

예를 들어,참인 유전적인 대다수 또는 복잡한 질병의 영양 결정 인자가 1.05 미만에 상대적 위험을 부여하면 유전적 또는 영양 역학은 크게 유토피아 노력이 될 것입니다.


- Corollary 3: The greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true. 

As shown above, the post-study probability that a finding is true (PPV) depends a lot on the pre-study odds (R). 

Thus, research findings are more likely true in confirmatory designs, such as large phase III randomized controlled trials, or meta-analyses thereof, than in hypothesis-generating experiments. 

Fields considered highly informative and creative given the wealth of the assembled and tested information, such as microarrays and other high-throughput discovery-oriented research [4,8,17], should have extremely low PPV.


- Corollary 3 : 과학 분야에서 테스트 된 관계의 선택의 수가 적거나 클때 연구 결과가 참 일 가능성이 적습니다.

위와 같이, 연구 결과가 참 인 경우 (PPV)인 사전 연구 확률은 많이 사전 연구 오즈비에 의존한다.( 여기서 오즈비는 어떤 사건이 일어날 가능성으로 p/(1-p)으로 표현됩니다, 성공확률/실패확률)

따라서 연구 결과는 가설 생성 실험에서보다 III 무작위 조절 시험 또는 그 메타 분석과 같은 확증 설계에서보다 실현 될 가능성이 더 큽니다.

분야는 매우 낮은 PPV를 가져야하는 마이크로 어레이 및 기타 고효율 발견 지향 연구 [4,8,17]와 같은 조립 및 검사 된 정보를 매우 유익하고 창조적인 것으로 간주됩니다. 

> 오즈비는 샘플링에서 생길 수 있는 편차를 최소화하여 , 통계적 의미를 강화합니다.


- Corollary 4: The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true. 

Flexibility increases the potential for transforming what would be “negative” results into “positive” results, i.e., bias, u. For several research designs, e.g., randomized controlled trials [18–20] or meta-analyses [21,22], there have been efforts to standardize their conduct and reporting. 

Adherence to common standards is likely to increase the proportion of true findings. 

The same applies to outcomes. 

True findings may be more common when outcomes are unequivocal and universally agreed (e.g., death) rather than when multifarious outcomes are devised (e.g., scales for schizophrenia outcomes) [23]. 

Similarly, fields that use commonly agreed, stereotyped analytical methods (e.g., Kaplan-Meier plots and the log-rank test) [24] may yield a larger proportion of true findings than fields where analytical methods are still under experimentation (e.g., artificial intelligence methods) and only “best” results are reported. 

Regardless, even in the most stringent research designs, bias seems to be a major problem. For example, there is strong evidence that selective outcome reporting, with manipulation of the outcomes and analyses reported, is a common problem even for randomized trails [25]. 

Simply abolishing selective publication would not make this problem go away.
