---
layout: single
title: "Time series forecasting with DeepAR - Synthetic data"
date: "2022-02-25 16:39:32 +0900"
last_modified_at: "2022-02-25 16:39:32 +0900"
---

# Time series forecasting with DeepAR - Synthetic data

DeepAR is a supervised learning algorithm for forecasting scalar time series. This notebook demonstrates how to prepare a dataset of time series for training DeepAR and how to use the trained model for inference.

This notebook was tested in Amazon SageMaker Studio on ml.t3.medium instance with Python 3 (Data Science) kernel.


```python
import time
import numpy as np
import pandas as pd
import json #JSON (JavaScript Object Notation) ì€ XML, YAML ê³¼ í•¨ê»˜ íš¨ìœ¨ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  êµí™˜(exchange data)í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„° í¬ë§·
import matplotlib.pyplot as plt

# Boto3ë¥¼ ì‚¬ìš©í•˜ë©´ Python ì• í”Œë¦¬ì¼€ì´ì…˜, ë¼ì´ë¸ŒëŸ¬ë¦¬ ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë¥¼ Amazon S3, Amazon EC2, Amazon DynamoDB ë“± AWS ì„œë¹„ìŠ¤ì™€ ì‰½ê²Œ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
import boto3
import sagemaker

# get_execution_role : ì´ˆê¸°ì— ì„¤ì •í•´ ë’€ë˜ IAM role ê°€ì ¸ì˜¤ê¸°
# IAM ì—­í•  : AWSì—ì„œ ìê²©ì¦ëª…í• ë•Œ í•„ìš”í•¨

from sagemaker import get_execution_role

np.random.seed(1)
```

Let's start by specifying:
- The S3 bucket prefix that you want to use for training and model data. Here we use the default bucket with `sagemaker_session.default_bucket()`, but you can change this to a bucket of your choosing. This should be within the same region as the Notebook Instance, training, and hosting.
- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Here we use the `get_execution_role` function to obtain the role arn which was specified when creating the notebook.


```python
prefix = "sagemaker/DEMO-deepar"

# ë°©ë¬¸ìê°€ ì›¹ ì„œë²„ì— ì ‘ì†í•´ ìˆëŠ” ìƒíƒœë¥¼ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ ë³´ê³  ê·¸ê²ƒì„ ì„¸ì…˜ì´ë¼ê³  í•œë‹¤.

sagemaker_session = sagemaker.Session()

role = get_execution_role() # IAM ìê²©ì¦ëª… ê°€ì ¸ì˜´

bucket = sagemaker_session.default_bucket() # S3 Bucket

s3_data_path = f"{bucket}/{prefix}/data" # S3 Bucket ë°ì´í„° ê²½ë¡œ ì„¤ì • 
s3_output_path = f"{bucket}/{prefix}/output"
```

Next, we configure the container image to be used for the region that we are running in.

# í›ˆë ¨ ì‘ì—… ìƒì„± ë° ì‹¤í–‰

ë‚´ì¥ Amazon SageMaker ì•Œê³ ë¦¬ì¦˜ì€ Amazon Elastic Container Registry(Amazon ECR)ì— Docker ì»¨í…Œì´ë„ˆë¡œ ì €ì¥ë©ë‹ˆë‹¤. ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ë ¤ë©´ ë¨¼ì € Amazon ECRì—ì„œ í•´ë‹¹ ë¦¬ì „ì— ê°€ì¥ ê°€ê¹Œìš´ forecasting-deepa ì»¨í…Œì´ë„ˆ ìœ„ì¹˜ë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.


```python
from sagemaker.amazon.amazon_estimator import get_image_uri

# ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
image_uri = get_image_uri(boto3.Session().region_name, "forecasting-deepar")
```

    The method get_image_uri has been renamed in sagemaker>=2.
    See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.
    Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.


### Generating and uploading data

In this example we want to train a model that can predict the next 48 points of syntheticly generated time series.
The time series that we use have hourly granularity.


```python
freq = "H" # hour ì‹œê°„ì„ ì˜ë¯¸í•œë‹¤. ì‹œê°„ìœ¼ë¡œ ì„¤ì •í•œë‹¤. 1ì‹œê°„ ê¸°ì¤€ì´ë‹¤.

# 48ì‹œê°„ ì´í›„ ì˜ˆì¸¡ í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¬ ê²ƒì´ë‹¤.
prediction_length = 48 # The prediction_length is fixed when a model is trained and it cannot be changed later
```

We also need to configure the so-called `context_length`, which determines how much context of the time series the model should take into account when making the prediction, i.e. how many previous points to look at. A typical value to start with is around the same size as the `prediction_length`. In our example we will use a longer `context_length` of `72`. Note that in addition to the `context_length` the model also takes into account the values of the time series at typical seasonal windows e.g. for hourly data the model will look at the value of the series 24h ago, one week ago one month ago etc. So it is not necessary to make the `context_length` span an entire month if you expect monthly seasonalities in your hourly data.

context_length	íŒŒë¼ë¯¸í„° ì´ë¦„ 

ë„¤íŠ¸ì›Œí¬ê°€ ì–´ëŠ ì‹œì ê¹Œì§€ ë³¼ ìˆ˜ ìˆëŠ”ì§€ ê²°ì •í•©ë‹ˆë‹¤. HPO í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ìŒì— ëŒ€í•œ ê°’ì„ ìë™ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.context_lengthëª¨ë¸ ì •í™•ë„ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” ë™ì‹œì— í›ˆë ¨ ì‹œê°„ì„ ê³ ë ¤í•©ë‹ˆë‹¤.

context_length í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ê³¼ê±° ë„¤íŠ¸ì›Œí¬ì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ ê±°ë¦¬ë¥¼ ì œì–´í•˜ë©°, prediction_length í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” í–¥í›„ ì˜ˆìƒí•  ìˆ˜ ìˆëŠ” ë²”ìœ„ë¥¼ ì œì–´í•©ë‹ˆë‹¤.
ê³¼ê±° 72ì¼ ë°ì´í„°ì…‹ íŒ¨í„´ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì´ ë³€ìˆ˜ ì„¤ì •í•¨


```python
context_length = 72
```

For this notebook, we will generate 200 noisy time series, each consisting of 400 data points and with seasonality of 24 hours. In our dummy example, all time series start at the same time point `t0`. When preparing your data, it is important to use the correct start point for each time series, because the model uses the time-point as a frame of reference, which enables it to learn e.g. that weekdays behave differently from weekends.


```python
t0 = "2016-01-01 00:00:00"# ì‹œì‘ì‹œê°„
data_length = 400 #ë°ì´í„° í–‰ ìˆ˜
num_ts = 200 # noisy time ì„¤ì •
period = 24 # 24ì‹œê°„
```

Each time series will be a noisy sine wave with a random level. 

> noise ì²˜ë¦¬

>> ë…¸ì´ì¦ˆ(noise)ë¼ê³ ë„ ë¶ˆë¦¬ëŠ” ì´ ë°ì´í„°ëŠ” ì¶”ì„¸, ê³„ì ˆì„± ë“±ìœ¼ë¡œ ì„¤ëª…ë˜ì§€ ì•Šì€ ë°ì´í„°ë¥¼ ì˜ë¯¸í•œë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤ë©´ ì˜ˆì¸¡ì˜ ì˜¤ì°¨ê°€ ì»¤ì§€ê¸° ë•Œë¬¸ì— ì „ì²˜ë¦¬ë¥¼ í†µí•´ì„œ ìµœëŒ€í•œ ì˜ˆì¸¡í•˜ëŠ”ë° ê´€ì—¬í•˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
ëœë¤í•¨ìˆ˜ ì ìš© ì‹œì¼œì¤€ë‹¤. ì¶”ì„¸ë¼ëŠ” ê²ƒì€ ë§ ê·¸ëŒ€ë¡œ ê²½í–¥ì„ ì˜ë¯¸í•œë‹¤. ì„¸ë¶€ì ì¸ ë°ì´í„°ë¥¼ ë‹¤ ë¹¼ê³  ì „ì²´ì ìœ¼ë¡œ ë³´ì•˜ì„ ë•Œ ì£¼ì‹ì´ ê°ì†Œí•˜ëŠ”ì§€ ì¦ê°€í•˜ëŠ”ì§€ ëŒ€ëµì ì¸ ì •ë³´ë¥¼ ë³´ì—¬ì¤€ë‹¤. ê³„ì ˆì„±(Seasonality)ì€ íŠ¹ì •í•œ ê¸°ê°„ë§ˆë‹¤ ì–´ë–¤ íŒ¨í„´ì„ ê°€ì§€ê³  ë°˜ë³µí•˜ëŠ”ì§€ í™•ì¸ í•  ìˆ˜ ìˆëŠ” íŠ¹ì„±ì´ë‹¤. ì´ ë°ì´í„°ë¥¼ í†µí•´ ì•ìœ¼ë¡œ ì–´ë–»ê²Œ ë³€í™”í•  ê²ƒì¸ì§€ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.
ì‹¤ì œ ë°ì´í„°ì—ëŠ” ë…¸ì´ì¦ˆë¥¼ ì˜ ì°¾ì•„ ì ìš©ì‹œì¼œì£¼ì–´ì•¼í•œë‹¤. ë°‘ì— í•¨ìˆ˜ëŠ” ì„ì˜ì ì¸ ë…¸ì´ì¦ˆ ë°ì´í„°ë¥¼ ì ìš©ì‹œì¼œ ì£¼ì—ˆë‹¤. 


```python
time_series = []
# num_ts = 200 # noisy time ì„¤ì • / ìœ„ì—ì„œ ì„¤ì •í•´ì¤€ê°’ 200 ë²”ìœ„ì—ì„œ kê°’ ë‚˜ì˜¤ê²Œ forë¬¸ ëŒë¦¼
for k in range(num_ts):  
    level = 10 * np.random.rand() # 0~1 ì‚¬ì´ì˜ ê°’ì„ 10ì„ ê³±í•´ ì§€ë©´ì„œ 1~10 ì‚¬ì´ì˜ ê°’ë“¤ì´ ë§Œë“¤ì–´ ì§‘ë‹ˆë‹¤.
    seas_amplitude = (0.1 + 0.3 * np.random.rand()) * level # y ì¶• ê°’ë“¤ì„ 2.5 ~ 6.0 ìœ¼ë¡œ ì„¤ì •í•¨
    sig = 0.05 * level  # noise parameter (constant in time)  
    time_ticks = np.array(range(data_length)) # ë°°ì—´ë¡œ ë§Œë“¬
    
    source = level + seas_amplitude * np.sin(time_ticks * (2 * np.pi) / period) #  ì‚¼ê°í•¨ìˆ˜ ì‚¬ì¸ ê°’ (trigonometric sine)ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ ì ìš©
    noise = sig * np.random.randn(data_length) # ì„ì˜ì ìœ¼ë¡œ ë…¸ì´ì¦ˆë¥¼ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤,
    data = source + noise # ì‹¤ì œ ë°ì´í„°ì—ëŠ” ë…¸ì´ì¦ˆê°€ í¬í•¨ëœë‹¤. ì„ì˜ì ìœ¼ë¡œ ì ìš©ì‹œì¼œì£¼ì—ˆìŠµë‹ˆë‹¤. 
    
    # ìœ„ì—ì„œ ì„¤ì •í•´ì¤€ ë³€ìˆ˜ t0 (ì‹œì‘ê°’), freq ë³€ìˆ˜ ì‹œê°„ìœ¼ë¡œ ì •í•œ ê°’ì„ ì ìš©ì‹œì¼œì¤ë‹ˆë‹¤.
    index = pd.date_range(start=t0, freq=freq, periods=data_length)
    
    time_series.append(pd.Series(data=data, index=index)) # ì‹œê³„ì—´ ë°ì´í„°ëŠ” series ë¥¼ ì ìš©ì‹œì¼œ ì¤Œ
```


```python
time_series[0].plot()
plt.show()
```

![output_20_0]({{ site.gdrive_url_prefix }}1xl7MV-v0HJVGUZ7E9F7QsZDk8ThobIg3)


Often one is interested in tuning or evaluating the model by looking at error metrics on a hold-out set. For other machine learning tasks such as classification, one typically does this by randomly separating examples into train/test sets. For forecasting it is important to do this train/test split in time rather than by series.

In this example, we will leave out the last section of each of the time series we just generated and use only the first part as training data. Here we will predict 48 data points, therefore we take out the trailing 48 points from each time series to define the training set. The test set contains the full range of each time series.


```python
time_series_training = [] # í›ˆë ¨ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ì–´ì¤Œ
for ts in time_series:
    time_series_training.append(ts[:-prediction_length]) # 48ì‹œê°„ ì´í›„ ì˜ˆì¸¡ í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ í›ˆë ¨ ì‹œí‚¤ê¸° ìœ„í•´ 48ì‹œê°„ ì „ê¹Œì§€ í›ˆë ¨ë°ì´í„°ë¡œ ë§Œë“¤ì–´ ì¤€ë‹¤.
```


```python
time_series[0].plot(label="test") # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‹œê°í™”
time_series_training[0].plot(label="train", ls=":") # .... ìœ¼ë¡œ í›ˆë ¨ë°ì´í„° ì‹œê°í™” í•´ì¤Œ
plt.legend()
plt.show()
```



![output_23_0]({{ site.gdrive_url_prefix }}1iA1jIj306Z3U4s50P5oGJBPVhErx4j1J)



The following utility functions convert `pandas.Series` objects into the appropriate JSON strings that DeepAR can consume. We will use these to write the data to S3.

# DeepAR preprocessing

 > DeepAR supports two types of data files: JSON Lines (one JSON object per line) and Parquet.

- start : A string with the format YYYY-MM-DD HH:MM:SS

- target : An array of floating-point values or integers that represent the time series. You can encode missing values as null literals, or as â€œNaNâ€ strings in JSON,

- cat(ì„ íƒ ì‚¬í•­) : An array of categorical features that can be used to encode the groups that the record belongs to. Categorical features must be encoded as a 0-based sequence of positive integers. For example, the categorical domain {R, G, B} can be encoded as {0, 1, 2}. All values from each categorical domain must be represented in the training dataset. That's because the DeepAR algorithm can forecast only for categories that have been observed during training.
 

## DeepAR ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ JSON í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ ì¤„ ìˆ˜ ìˆëŠ” í•¨ìˆ˜ ìƒì„±

- JSON 


1. XML, YAML ê³¼ í•¨ê»˜ íš¨ìœ¨ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  êµí™˜í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„° í¬ë§·

2. êµ¬ì¡° 

  2.1 ì´ë¦„/ê°’ ìŒì˜ ì§‘í•© (A collection of name/value pairs): object, record, struct, dictionary, hash table, keyed list, associative array
  
  2.2 ì •ë ¬ëœ ê°’ì˜ ë¦¬ìŠ¤íŠ¸ (An ordered list of values): array, vector, list, sequence


    
![20220222_185322_1]({{ site.gdrive_url_prefix }}1xFdlRdihlNE0MSqx08SXgUBe_8enNnHZ)




- ì‚¬ì§„ ì¶œì²˜ :[https://rfriend.tistory.com/474](https://rfriend.tistory.com/474)

"https://ai-example.notebook.ap-northeast-2.sagemaker.aws/view/amazon-sagemaker-examples/synthetic_data/data/20220222_185322_1.png"


```python
def series_to_obj(ts, cat=None):
    ''' 
    obj ë³€ìˆ˜ëŠ” dictìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. python objectë¥¼ jsonìœ¼ë¡œ ë³€í™˜ì‹œí‚¤ê¸° ìœ„í•´ 
    json.dumps í•¨ìˆ˜ ì ìš©ì‹œí‚´  
    '''
    obj = {"start": str(ts.index[0]), "target": list(ts)} # startëŠ” string ì´ì—¬ì•¼ í•¨, tsëŠ” ë°ì´í„°ë¥¼ ë‹´ì€ ë³€ìˆ˜ì´ë‹¤.
    if cat is not None: # cat default ê°’ì´ Noneì¸ë° None ì´ ì•„ë‹ë•Œ ì‹¤í–‰ë¨
        obj["cat"] = cat
    return obj

# json í˜•íƒœë¡œ ë°˜í™˜ í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜
def series_to_jsonline(ts, cat=None):
    return json.dumps(series_to_obj(ts, cat))
```

** client **

1. low-level ì¸í„°í˜ì´ìŠ¤

2. service descriptionì— ì˜í•´ ë§Œë“¤ì–´ì§

3. botocore ìˆ˜ì¤€ì˜ clientë¥¼ ê³µê°œ(botocoreëŠ” AWS CLIì™€ boto3ì˜ ê¸°ì´ˆê°€ ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬)

4. AWS APIì™€ 1:1 ë§¤í•‘ë¨
    
5. ë©”ì†Œë“œê°€ ìŠ¤ë„¤ì´í¬ ì¼€ì´ìŠ¤ë¡œ ì •ì˜ë˜ì–´ ìˆìŒ


```python
encoding = "utf-8"
FILE_TRAIN = "train.json"
FILE_TEST = "test.json"

# í›ˆë ¨ë°ì´í„° json í˜•íƒœë¡œ ì €ì¥
# ë°”ì´ë„ˆë¦¬ íŒŒì¼í˜•íƒœë¡œ ì¨ì£¼ê³  ì €ì¥í•œë‹¤.
with open(FILE_TRAIN, "wb") as f:
    for ts in time_series_training:
        f.write(series_to_jsonline(ts).encode(encoding)) #json í˜•íƒœë¡œ ë°˜í™˜ í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ ì ìš©/ encode ì ìš© 
        f.write("\n".encode(encoding))

# í…ŒìŠ¤íŠ¸ë°ì´í„° json í˜•íƒœë¡œ ì €ì¥
with open(FILE_TEST, "wb") as f:
    for ts in time_series:
        f.write(series_to_jsonline(ts).encode(encoding))
        f.write("\n".encode(encoding))

# AWS s3ì¸ ë²„í‚·ì— íŒŒì¼ ì—…ë¡œë“œ í•¨        
s3 = boto3.client("s3")

# prefix = "sagemaker/DEMO-deepar" ìœ„ì—ì„œ ê²½ë¡œ ì„¤ì •í•œê±° ë³€ìˆ˜ ë„£ì–´ì¤Œ
s3.upload_file(FILE_TRAIN, bucket, prefix + "/data/train/" + FILE_TRAIN)
s3.upload_file(FILE_TEST, bucket, prefix + "/data/test/" + FILE_TRAIN)
```

### Train a model

We can now define the estimator that will launch the training job.

- ì¶œì²˜ : [https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)

1. sagemaker_session (sagemaker.session.Session) â€“ Session object which manages interactions with Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one using the default AWS configuration chain.

2. image_uri (str) â€“ The container image to use for training.

3. role (str) â€“ An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs that create Amazon SageMaker endpoints use this role to access training data and model artifacts. After the endpoint is created, the inference code might use the IAM role, if it needs to access an AWS resource.

4. instance_count (int) â€“ Number of Amazon EC2 instances to use for training.

- What is an Amazon EC2 instance?

An Amazon EC2 instance is a virtual server in Amazon's Elastic Compute Cloud (EC2) for running applications on the Amazon Web Services (AWS) infrastructure. AWS is a comprehensive, evolving cloud computing platform; EC2 is a service that enables business subscribers to run application programs in the computing environment. It can serve as a practically unlimited set of virtual machines (VMs).

5. base_job_name (str) â€“ Prefix for training job name when the fit() method launches. If not specified, the estimator generates a default job name, based on the training image name and current timestamp.




```python
estimator = sagemaker.estimator.Estimator(
    sagemaker_session=sagemaker_session,
    image_uri=image_uri, # í›ˆë ¨ í•˜ê¸° ìœ„í•´ ì„¤ì •í•¨, ìœ„ì—ì„œ ë³€ìˆ˜ë¡œ ì„¤ì •í•œ forecasting-deeparìœ¼ë¡œ í›ˆë ¨ì‹œí‚´
    role=role,  # AWS IAM role , ìœ„ì—ì„œ ë³€ìˆ˜ë¡œ ì„¤ì •í•œ IAM ìê²©ì¦ëª… ê°€ì ¸ì˜´
    instance_count=1, # EC2 instances ìˆ«ì , ê°€ìƒí™˜ê²½ ëª‡ê°œë¥¼ ì‚¬ìš©í• ì§€ ì„¤ì •í•¨, ê°€ìƒí™˜ê²½ 1ê°œë¥¼ ì‚¬ìš©í•¨.
    instance_type="ml.m5.2xlarge", # ë…¸íŠ¸ë¶ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±í• ë•Œ ì„¤ì • í–ˆë˜ ë…¸íŠ¸ë¶ ì¸ìŠ¤í„´ìŠ¤ ìœ í˜• ml.m5.2xlargeìœ¼ë¡œ ì„¤ì •
    base_job_name="DEMO-deepar", # deepar default ê°’ 
    output_path=f"s3://{s3_output_path}",  # ìœ„ì—ì„œ ì„¤ì •í•œ ë³€ìˆ˜ f"{bucket}/{prefix}/output" ë„£ì–´ì¤Œ
)
```

Next we need to set some hyperparameters: for example, frequency of the time series used, number of data points the model will look at in the past, number of predicted data points. The other hyperparameters concern the model to train (number of layers, number of cells per layer, likelihood function) and the training options such as number of epochs, batch size, and learning rate. Refer to the documentation for a full description of the available parameters.

* DeepAR Parameters

freq: Frequency at which data is sampled , for our task we have sampled it monthwise

prediction_length: Corresponds to how many time steps into the future we have to make predictions , in this case it 10 months in future

context_length: Corresponds to how many observations we will be looking back while making future preds in this case 42 months

use_feat_static_cat: This is one of the most important parameter while doing Multiple Time Series we need some indicator variable to tell our model which time series observation is which , we earlier had made variable ts_code this stores the item_id in form of integers which are used to tell our model at run time to which time series an observation belongs to hence we give use_feat_static_cat as True

cardinality: While making preds we are only using the returns observation for each item month wise and no other extra features , this is cardinality , hence it is 1

num_layers: Corresponds to number of RNN Layers in our NN

num_cells: Corresponds to number of cells in each RNN

cell_type: What is cell_type here it is LSTM which is RNN , we can also use GRU

Trainer: A Trainer object to be used , currently using Default Trainer Object with 15 epochs

- mini_batch_size

- ì¶œì²˜:[https://welcome-to-dewy-world.tistory.com/86](https://welcome-to-dewy-world.tistory.com/86)

ë°ì´í„°ë¥¼ 1ê°œì”© ì…ë ¥ë°›ì•„ ì´ 10ë§Œë²ˆì˜ ì—°ì‚°ì„ ì§„í–‰í•˜ëŠ” ê²ƒë³´ë‹¤ í•œë²ˆì— í° ë¬¶ìŒìœ¼ë¡œ ë°ì´í„°ë¡œ ì…ë ¥ë°›ì•„(Batch) në²ˆì˜ ì—°ì‚°ì„ ì§„í–‰í•˜ëŠ” ê²ƒì´ ë” ë¹ ë¥´ë‹¤. ì¦‰, ëŠë¦° I/Oë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ì½ëŠ” íšŸìˆ˜ë¥¼ ì¤„ì´ê³  CPUë‚˜ GPUë¡œ ìˆœìˆ˜ ê³„ì‚°ì„ í•˜ëŠ” ë¹„ìœ¨ì„ ë†’ì—¬ ì†ë„ë¥¼ ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆë‹¤ëŠ” ëœ»ì´ë‹¤. ë˜í•œ ìˆ˜ì¹˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬ ëŒ€ë¶€ë¶„ì´ í° ë°°ì—´ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ê³ ë„ë¡œ ìµœì í™”ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— Batch(ë¬¶ìŒ)ë¡œ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ì†ë„ ì¸¡ë©´ì—ì„œ íš¨ìœ¨ì ì´ë‹¤.

- ë¯¸ë‹ˆë°°ì¹˜ (Mini-Batch)

ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ì„¤ëª…í•˜ê¸°ì— ì•ì„œ ë°ì´í„°ë¥¼ í•˜ë‚˜ì”© í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ê³¼ ì „ì²´ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì˜ ì¥ë‹¨ì ì— ëŒ€í•´ ì†Œê°œí•˜ê² ë‹¤.

 

ë°ì´í„°ë¥¼ í•˜ë‚˜ í•˜ë‚˜ì”© ì‹ ê²½ë§ì— ë„£ì–´ í•™ìŠµì„ ì‹œí‚¤ë©´ ìš°ì„  ì¥ì ìœ¼ë¡œëŠ” ì‹ ê²½ë§ì„ í•œë²ˆ í•™ìŠµì‹œí‚¤ëŠ”ë° ì†Œìš”ë˜ëŠ” ì‹œê°„ì´ ë§¤ìš° ì§§ë‹¤ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ë°ì´í„°ë¥¼ í•˜ë‚˜ì”© ì…ë ¥ë°›ìœ¼ë©´ ê³„ì‚° ì†ë„ê°€ ì‚¬ì‹¤ìƒ ëŠ¦ì–´ì§„ë‹¤. ìš°ë¦¬ëŠ” ëŒ€ë¶€ë¶„ ë”¥ëŸ¬ë‹ì— GPUë¥¼ ë§ì´ ì‚¬ìš©í•˜ëŠ”ë° ê·¸ ì´ìœ ê°€ GPUì˜ ë³‘ë ¬ì²˜ë¦¬ì— ìˆë‹¤. í•˜ì§€ë§Œ ë°ì´í„°ë¥¼ í•œê°œì”© ë„£ì–´ ì¸ê³µì§€ëŠ¥ì„ í•™ìŠµì‹œí‚¤ë©´ ì‚¬ì‹¤ìƒ GPUì˜ ë³‘ë ¬ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë‹ˆ ê·¸ë§Œí¼ ìì› ë‚­ë¹„ê°€ ëœë‹¤. ë˜í•œ ì˜¤ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” Loss Functionì—ì„œ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ëŠ”ë° ìƒë‹¹íˆ ë§ì´ í—¤ë§¤ê²Œ ëœë‹¤.

 

ì „ì²´ ë°ì´í„°ë¥¼ ì…ë ¥í•˜ëŠ” ê²½ìš°ëŠ” í•œë²ˆì— ì—¬ëŸ¬ê°œì˜ ë°ì´í„°ì— ëŒ€í•´ì„œ ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜¤ì°¨ë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” cost functionì˜ ìµœì ì˜ parameterë¥¼ í•˜ë‚˜ì”© í•™ìŠµí•˜ëŠ”ê²ƒë³´ë‹¤ ë¹ ë¥´ê²Œ ì•Œì•„ë‚¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì „ì²´ ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¤ê¸° ë•Œë¬¸ì— ì‹ ê²½ë§ì„ í•œ ë²ˆ í•™ìŠµì‹œí‚¤ëŠ”ë° ì†Œìš”ë˜ëŠ” ì‹œê°„ì´ ë§¤ìš° ê¸¸ë‹¤.

 

ìœ„ì˜ ë¬¸ì œì ê³¼ ì¥ì ì„ ì ì ˆíˆ ì„ì€ ê²ƒì´ ë°”ë¡œ ë¯¸ë‹ˆ ë°°ì¹˜ì´ë‹¤. ë¯¸ë‹ˆ ë°°ì¹˜ëŠ” SGD(Stochastic Gradient Descent : í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•)ì™€ ë°°ì¹˜ë¥¼ ì„ì€ ê²ƒìœ¼ë¡œ ì „ì²´ ë°ì´í„°ë¥¼ Në“±ë¶„í•˜ì—¬ ê°ê°ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ ë°°ì¹˜ ë°©ì‹ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ë‹¤. ë”°ë¼ì„œ ìµœëŒ€í•œ ì‹ ê²½ë§ì„ í•œ ë²ˆ í•™ìŠµì‹œí‚¤ëŠ”ë°(iteration) ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì¤„ì´ë©´ì„œ ì „ì²´ ë°ì´í„°ë¥¼ ë°˜ì˜í•  ìˆ˜ ìˆê²Œë˜ë©° íš¨ìœ¨ì ìœ¼ë¡œ CPUì™€ GPUë¥¼ í™œìš©í•  ìˆ˜ ìˆê²Œ ëœë‹¤.

- í•™ìŠµë¥ (Learning Rate)

- ì¶œì²˜:[https://codingrabbit.tistory.com/11](https://codingrabbit.tistory.com/11)   

ì•ì˜ ê²½ì‚¬í•˜ê°•ë²•ì—ì„œ ì‹œì‘ì ì—ì„œ ë‹¤ìŒì ìœ¼ë¡œ ì´ë™í•  ë•Œ ë³´í­(í•™ìŠµë¥ ) ë§Œí¼ ì´ë™í•œ ì ì„ ë‹¤ìŒì ìœ¼ë¡œ ì •í•œë‹¤ê³  í•˜ì˜€ìŠµë‹ˆë‹¤. í•™ìŠµë¥ ì¸ë€, í˜„ì¬ì ì—ì„œ ë‹¤ìŒì ìœ¼ë¡œ ì–¼ë§Œí¼ ì´ë™í• ì§€, ë‹¤ë¥´ê²Œ ë§í•˜ë©´ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì„¸ì„¸í•˜ê²Œ í•™ìŠµì„ í• ì§€ë¥¼ ë§í•©ë‹ˆë‹¤.



í•™ìŠµë¥ ì´ ì‘ë‹¤ë©´ ì†ì‹¤ì´ ìµœì ì¸ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ëŠ”ë° ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦´ ê²ƒì´ë©°, í•™ìŠµë¥ ì´ ë„ˆë¬´ í¬ë‹¤ë©´ ìµœì ì ì„ ë¬´ì§ˆì„œí•˜ê²Œ ì´íƒˆí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.


![20220225_141334_1]({{ site.gdrive_url_prefix }}1RDdWaDCj9VUaIabQdtqilNazFJClxDM3)


- ë“œë¡­ì•„ì›ƒ 

ë“œë¡­ì•„ì›ƒ(dropout)ì€ ì‹ ê²½ë§ì—ì„œ ê°€ì¥ íš¨ê³¼ì ì´ê³  ë„ë¦¬ ì‚¬ìš©í•˜ëŠ” ê·œì œ ê¸°ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. í† ë¡ í† (Toronto) ëŒ€í•™ì˜ íŒíŠ¼(Hinton)ê³¼ ê·¸ì˜ ì œìë“¤ì´ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ë“œë¡­ì•„ì›ƒì„ ì¸µì— ì ìš©í•˜ë©´ í›ˆë ¨í•˜ëŠ” ë™ì•ˆ ì¸µì˜ ì¶œë ¥ íŠ¹ì„±ì„ ëœë¤í•˜ê²Œ ë•ë‹ˆë‹¤(ì¦‰, 0ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤). í›ˆë ¨í•˜ëŠ” ë™ì•ˆ ì–´ë–¤ ì…ë ¥ ìƒ˜í”Œì— ëŒ€í•´ [0.2, 0.5, 1.3, 0.8, 1.1] ë²¡í„°ë¥¼ ì¶œë ¥í•˜ëŠ” ì¸µì´ ìˆë‹¤ê³  ê°€ì •í•´ ë³´ì£ . ë“œë¡­ì•„ì›ƒì„ ì ìš©í•˜ë©´ ì´ ë²¡í„°ì—ì„œ ëª‡ ê°œì˜ ì›ì†Œê°€ ëœë¤í•˜ê²Œ 0ì´ ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, [0, 0.5, 1.3, 0, 1.1]ê°€ ë©ë‹ˆë‹¤. "ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨"ì€ 0ì´ ë˜ëŠ” íŠ¹ì„±ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤. ë³´í†µ 0.2ì—ì„œ 0.5 ì‚¬ì´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œëŠ” ì–´ë–¤ ìœ ë‹›ë„ ë“œë¡­ì•„ì›ƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í›ˆë ¨ ë‹¨ê³„ë³´ë‹¤ ë” ë§ì€ ìœ ë‹›ì´ í™œì„±í™”ë˜ê¸° ë•Œë¬¸ì— ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•´ ì¸µì˜ ì¶œë ¥ ê°’ì„ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ë§Œí¼ ì¤„ì…ë‹ˆë‹¤.


ë“œë¡­ì•„ì›ƒì„ ì¶”ê°€í•˜ë©´ ê¸°ì¤€ ëª¨ë¸ë³´ë‹¤ í™•ì‹¤íˆ í–¥ìƒëœë‹¤.

ì •ë¦¬í•˜ë©´ ì‹ ê²½ë§ì—ì„œ ê³¼ëŒ€ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

ë” ë§ì€ í›ˆë ¨ ë°ì´í„°ë¥¼ ëª¨ìë‹ˆë‹¤.

ë„¤íŠ¸ì›Œí¬ì˜ ìš©ëŸ‰ì„ ì¤„ì…ë‹ˆë‹¤.

ê°€ì¤‘ì¹˜ ê·œì œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

ë“œë¡­ì•„ì›ƒì„ ì¶”ê°€í•©ë‹ˆë‹¤.

- Early Stopping ì´ë€ ë¬´ì—‡ì¸ê°€? 



ë”¥ëŸ¬ë‹ì„ ë¹„ë¡¯í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ í•œ ê°€ì§€ ì¤‘ìš”í•œ ë”œë ˆë§ˆëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. 



ë„ˆë¬´ ë§ì€ Epoch ì€ overfitting ì„ ì¼ìœ¼í‚¨ë‹¤. í•˜ì§€ë§Œ ë„ˆë¬´ ì ì€ Epoch ì€ underfitting ì„ ì¼ìœ¼í‚¨ë‹¤. 



ì´ëŸ° ìƒí™©ì—ì„œ Epoch ì„ ì–´ë–»ê²Œ ì„¤ì •í•´ì•¼í•˜ëŠ”ê°€? 

Epoch ì„ ì •í•˜ëŠ”ë° ë§ì´ ì‚¬ìš©ë˜ëŠ” Early stopping ì€ ë¬´ì¡°ê±´ Epoch ì„ ë§ì´ ëŒë¦° í›„, íŠ¹ì • ì‹œì ì—ì„œ ë©ˆì¶”ëŠ” ê²ƒì´ë‹¤. 



ê·¸ íŠ¹ì •ì‹œì ì„ ì–´ë–»ê²Œ ì •í•˜ëŠëƒê°€ Early stopping ì˜ í•µì‹¬ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ hold-out validation set ì—ì„œì˜ ì„±ëŠ¥ì´ ë”ì´ìƒ ì¦ê°€í•˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì„ ì¤‘ì§€ì‹œí‚¤ê²Œ ëœë‹¤.   





```python
hyperparameters = {
    "time_freq": freq, # ìœ„ì—ì„œ ì„¤ì •í•œ ë³€ìˆ˜ freq = "H" # hour ì‹œê°„ì„ ì˜ë¯¸í•œë‹¤. 
    "context_length": str(context_length),  # context_length = 72ìœ¼ë¡œ ìœ„ì—ì„œ ì„¤ì •í•œ ë³€ìˆ˜ ê°’ì„ ë„£ì–´ì¤Œ
    "prediction_length": str(prediction_length),   # prediction_length = 48
    "num_cells": "40",  # RNN cellsì˜ ìˆ˜ë¥¼ 40ìœ¼ë¡œ ì„¤ì •í•¨
    "num_layers": "3", # RNN Layersì˜ ìˆ˜ë¥¼ 3ìœ¼ë¡œ ì„¤ì •í•¨
    "likelihood": "gaussian", # Gaussian model: meanê³¼ varianceë¥¼ neural networksì˜ outputìœ¼ë¡œ ë‘ê³  varianceê°€ ì–‘ì´ ë˜ë„ë¡ variance outputì— softplusë¥¼ ì ìš©í•œë‹¤. MSEë¥¼ êµ¬í•˜ëŠ” ê²ƒê³¼ ê±°ì˜ ê°™ìœ¼ë‚˜ ë¶„ëª¨ì— variance termì´ ë¶™ê¸° ë•Œë¬¸ì— training ì•ˆì •ì„±ì€ ì¡°ê¸ˆ ë–¨ì–´ì§€ëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ
    "epochs": "20", # ì „ì²´ ë°ì´í„° 20ë²ˆ í•™ìŠµì‹œí‚´
    "mini_batch_size": "32", #  ë¯¸ë‹ˆ ë°°ì¹˜(mini batch): ì „ì²´ ë°ì´í„° ì…‹ì„ ëª‡ ê°œì˜ ë°ì´í„° ì…‹ìœ¼ë¡œ ë‚˜ëˆ„ì—ˆì„ ë•Œ, ê·¸ ì‘ì€ ë°ì´í„° ì…‹ ë­‰ì¹˜ / 32ê°œì”© ë‚˜ëˆ„ì–´ì¨ ê³„ì‚°í•œë‹¤.
    "learning_rate": "0.001", # ì ì ˆí•œ í•™ìŠµë¥ ì„ ì„¤ì •í•¨
    "dropout_rate": "0.05", # 0ì´ ë˜ëŠ” íŠ¹ì„±ì˜ ë¹„ìœ¨ì„ ì„¤ì •í•¨
    "early_stopping_patience": "10", 
}
```


```python
estimator.set_hyperparameters(**hyperparameters) # estimatorì— hyperparameters ì ìš©ì‹œí‚´
```

We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.

If you provide the `test` data channel, as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test data set. This is done by predicting the last `prediction_length` points of each time series in the test set and comparing this to the actual value of the time series. The computed error metrics will be included as part of the log output.

**Note:** the next cell may take a few minutes to complete, depending on data size, model complexity, and training options.


```python
data_channels = {"train": f"s3://{s3_data_path}/train/", "test": f"s3://{s3_data_path}/test/"}

estimator.fit(inputs=data_channels) # estimatorì— train , test ëª¨ë¸ ì ìš©ì‹œí‚´
```

    2022-02-25 07:08:10 Starting - Starting the training job...
    2022-02-25 07:08:34 Starting - Preparing the instances for trainingProfilerReport-1645772890: InProgress
    ......
    2022-02-25 07:09:34 Downloading - Downloading input data
    2022-02-25 07:09:34 Training - Downloading the training image........[34mArguments: train[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736 integration.py:592] worker started[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '72', 'dropout_rate': '0.05', 'early_stopping_patience': '10', 'epochs': '20', 'learning_rate': '0.001', 'likelihood': 'gaussian', 'mini_batch_size': '32', 'num_cells': '40', 'num_layers': '3', 'prediction_length': '48', 'time_freq': 'H'}[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.05', 'early_stopping_patience': '10', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'gaussian', 'mini_batch_size': '32', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '3', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '72', 'epochs': '20', 'prediction_length': '48', 'time_freq': 'H'}[0m
    [34mProcess 1 is a worker.[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] Detected entry point for worker worker[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] Using early stopping with patience 10[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] random_seed is None[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] Training set statistics:[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] Real time series[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] number of time series: 200[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] number of observations: 70400[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] mean target length: 352.0[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] min/mean/max target: 0.0040314337238669395/4.920437259127132/14.95400619506836[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] mean abs(target): 4.920437259127132[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] contains missing values: no[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] Small number of time series. Doing 2 passes over dataset with prob 0.8 per epoch.[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] Test set statistics:[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] Real time series[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] number of time series: 200[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] number of observations: 80000[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] mean target length: 400.0[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] min/mean/max target: 0.0040314337238669395/4.917104725933075/15.022744178771973[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] mean abs(target): 4.917104725933075[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] contains missing values: no[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] #memory_usage::<batchbuffer> = 3.5982131958007812 mb[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] nvidia-smi took: 0.02516651153564453 secs to identify 0 gpus[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] Number of GPUs being used: 0[0m
    [34m[02/25/2022 07:10:48 INFO 139844299908736] Create Store: local[0m
    [34m#metrics {"StartTime": 1645773048.9825969, "EndTime": 1645773049.2780855, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"get_graph.time": {"sum": 294.175386428833, "count": 1, "min": 294.175386428833, "max": 294.175386428833}}}[0m
    [34m[02/25/2022 07:10:49 INFO 139844299908736] Number of GPUs being used: 0[0m
    [34m[02/25/2022 07:10:49 INFO 139844299908736] #memory_usage::<model> = 38 mb[0m
    [34m#metrics {"StartTime": 1645773049.278172, "EndTime": 1645773049.7575011, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"initialize.time": {"sum": 774.7776508331299, "count": 1, "min": 774.7776508331299, "max": 774.7776508331299}}}[0m
    [34m[02/25/2022 07:10:50 INFO 139844299908736] Epoch[0] Batch[0] avg_epoch_loss=2.316095[0m
    [34m[02/25/2022 07:10:50 INFO 139844299908736] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=2.3160951137542725[0m
    [34m[02/25/2022 07:10:50 INFO 139844299908736] Epoch[0] Batch[5] avg_epoch_loss=1.583442[0m
    [34m[02/25/2022 07:10:50 INFO 139844299908736] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=1.5834423104921977[0m
    [34m[02/25/2022 07:10:50 INFO 139844299908736] Epoch[0] Batch [5]#011Speed: 207.89 samples/sec#011loss=1.583442[0m
    [34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration
      self.gen.throw(type, value, traceback)[0m
    [34m[02/25/2022 07:10:51 INFO 139844299908736] processed a total of 292 examples[0m
    [34m#metrics {"StartTime": 1645773049.7575946, "EndTime": 1645773051.5706682, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"epochs": {"sum": 20.0, "count": 1, "min": 20, "max": 20}, "update.time": {"sum": 1812.9818439483643, "count": 1, "min": 1812.9818439483643, "max": 1812.9818439483643}}}[0m
    [34m[02/25/2022 07:10:51 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=161.05025335033514 records/second[0m
    [34m[02/25/2022 07:10:51 INFO 139844299908736] #progress_metric: host=algo-1, completed 5.0 % of epochs[0m
    [34m[02/25/2022 07:10:51 INFO 139844299908736] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.3728791713714599[0m
    [34m[02/25/2022 07:10:51 INFO 139844299908736] best epoch loss so far[0m
    [34m[02/25/2022 07:10:51 INFO 139844299908736] Saved checkpoint to "/opt/ml/model/state_2ebbf82d-11a0-4672-b3bc-83154799b58c-0000.params"[0m
    [34m#metrics {"StartTime": 1645773051.5707483, "EndTime": 1645773051.6205406, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"state.serialize.time": {"sum": 49.318552017211914, "count": 1, "min": 49.318552017211914, "max": 49.318552017211914}}}[0m
    [34m[02/25/2022 07:10:51 INFO 139844299908736] Epoch[1] Batch[0] avg_epoch_loss=1.293630[0m
    [34m[02/25/2022 07:10:51 INFO 139844299908736] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=1.2936302423477173[0m
    [34m[02/25/2022 07:10:52 INFO 139844299908736] Epoch[1] Batch[5] avg_epoch_loss=1.044226[0m
    [34m[02/25/2022 07:10:52 INFO 139844299908736] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=1.0442256530125935[0m
    [34m[02/25/2022 07:10:52 INFO 139844299908736] Epoch[1] Batch [5]#011Speed: 233.59 samples/sec#011loss=1.044226[0m
    [34m[02/25/2022 07:10:53 INFO 139844299908736] processed a total of 297 examples[0m
    [34m#metrics {"StartTime": 1645773051.6206155, "EndTime": 1645773053.1967614, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1576.082706451416, "count": 1, "min": 1576.082706451416, "max": 1576.082706451416}}}[0m
    [34m[02/25/2022 07:10:53 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=188.420168280764 records/second[0m
    [34m[02/25/2022 07:10:53 INFO 139844299908736] #progress_metric: host=algo-1, completed 10.0 % of epochs[0m
    [34m[02/25/2022 07:10:53 INFO 139844299908736] #quality_metric: host=algo-1, epoch=1, train loss <loss>=0.88489910364151[0m
    [34m[02/25/2022 07:10:53 INFO 139844299908736] best epoch loss so far[0m
    [34m[02/25/2022 07:10:53 INFO 139844299908736] Saved checkpoint to "/opt/ml/model/state_4f8887dd-f693-470d-9ee7-6edae14c18d1-0000.params"[0m
    [34m#metrics {"StartTime": 1645773053.1969085, "EndTime": 1645773053.2513883, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"state.serialize.time": {"sum": 54.056644439697266, "count": 1, "min": 54.056644439697266, "max": 54.056644439697266}}}[0m
    [34m[02/25/2022 07:10:53 INFO 139844299908736] Epoch[2] Batch[0] avg_epoch_loss=0.981265[0m
    [34m[02/25/2022 07:10:53 INFO 139844299908736] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=0.9812645316123962[0m
    [34m[02/25/2022 07:10:54 INFO 139844299908736] Epoch[2] Batch[5] avg_epoch_loss=0.622232[0m
    [34m[02/25/2022 07:10:54 INFO 139844299908736] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=0.6222319304943085[0m
    [34m[02/25/2022 07:10:54 INFO 139844299908736] Epoch[2] Batch [5]#011Speed: 208.31 samples/sec#011loss=0.622232[0m
    [34m[02/25/2022 07:10:54 INFO 139844299908736] processed a total of 308 examples[0m
    [34m#metrics {"StartTime": 1645773053.2514575, "EndTime": 1645773054.9533012, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1701.7793655395508, "count": 1, "min": 1701.7793655395508, "max": 1701.7793655395508}}}[0m
    [34m[02/25/2022 07:10:54 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=180.97549772457756 records/second[0m
    [34m[02/25/2022 07:10:54 INFO 139844299908736] #progress_metric: host=algo-1, completed 15.0 % of epochs[0m
    [34m[02/25/2022 07:10:54 INFO 139844299908736] #quality_metric: host=algo-1, epoch=2, train loss <loss>=0.5216166645288467[0m
    [34m[02/25/2022 07:10:54 INFO 139844299908736] best epoch loss so far[0m
    [34m[02/25/2022 07:10:55 INFO 139844299908736] Saved checkpoint to "/opt/ml/model/state_29296b5e-2bda-4a5a-8f43-bcadbf5311bb-0000.params"[0m
    [34m#metrics {"StartTime": 1645773054.9533758, "EndTime": 1645773055.0068238, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"state.serialize.time": {"sum": 52.91628837585449, "count": 1, "min": 52.91628837585449, "max": 52.91628837585449}}}[0m
    [34m[02/25/2022 07:10:55 INFO 139844299908736] Epoch[3] Batch[0] avg_epoch_loss=0.183683[0m
    [34m[02/25/2022 07:10:55 INFO 139844299908736] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=0.1836828589439392[0m
    [34m[02/25/2022 07:10:56 INFO 139844299908736] Epoch[3] Batch[5] avg_epoch_loss=0.255676[0m
    [34m[02/25/2022 07:10:56 INFO 139844299908736] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=0.25567640736699104[0m
    [34m[02/25/2022 07:10:56 INFO 139844299908736] Epoch[3] Batch [5]#011Speed: 210.57 samples/sec#011loss=0.255676[0m
    [34m[02/25/2022 07:10:56 INFO 139844299908736] processed a total of 310 examples[0m
    [34m#metrics {"StartTime": 1645773055.0068958, "EndTime": 1645773056.6385052, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1631.5429210662842, "count": 1, "min": 1631.5429210662842, "max": 1631.5429210662842}}}[0m
    [34m[02/25/2022 07:10:56 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=189.991479686629 records/second[0m
    [34m[02/25/2022 07:10:56 INFO 139844299908736] #progress_metric: host=algo-1, completed 20.0 % of epochs[0m
    [34m[02/25/2022 07:10:56 INFO 139844299908736] #quality_metric: host=algo-1, epoch=3, train loss <loss>=0.26868246123194695[0m
    [34m[02/25/2022 07:10:56 INFO 139844299908736] best epoch loss so far[0m
    [34m[02/25/2022 07:10:56 INFO 139844299908736] Saved checkpoint to "/opt/ml/model/state_ea0705a7-9797-41a2-8b82-2a3559eeccd6-0000.params"[0m
    [34m#metrics {"StartTime": 1645773056.638581, "EndTime": 1645773056.6932905, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"state.serialize.time": {"sum": 54.305315017700195, "count": 1, "min": 54.305315017700195, "max": 54.305315017700195}}}[0m
    [34m[02/25/2022 07:10:56 INFO 139844299908736] Epoch[4] Batch[0] avg_epoch_loss=0.131494[0m
    [34m[02/25/2022 07:10:56 INFO 139844299908736] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=0.13149385154247284[0m
    [34m[02/25/2022 07:10:57 INFO 139844299908736] Epoch[4] Batch[5] avg_epoch_loss=0.188895[0m
    [34m[02/25/2022 07:10:57 INFO 139844299908736] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=0.18889484911536178[0m
    [34m[02/25/2022 07:10:57 INFO 139844299908736] Epoch[4] Batch [5]#011Speed: 227.85 samples/sec#011loss=0.188895[0m
    [34m[02/25/2022 07:10:58 INFO 139844299908736] Epoch[4] Batch[10] avg_epoch_loss=0.111592[0m
    [34m[02/25/2022 07:10:58 INFO 139844299908736] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=0.018827971816062928[0m
    [34m[02/25/2022 07:10:58 INFO 139844299908736] Epoch[4] Batch [10]#011Speed: 206.63 samples/sec#011loss=0.018828[0m
    [34m[02/25/2022 07:10:58 INFO 139844299908736] processed a total of 325 examples[0m
    [34m#metrics {"StartTime": 1645773056.6933594, "EndTime": 1645773058.4578137, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1764.390468597412, "count": 1, "min": 1764.390468597412, "max": 1764.390468597412}}}[0m
    [34m[02/25/2022 07:10:58 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=184.1876428041568 records/second[0m
    [34m[02/25/2022 07:10:58 INFO 139844299908736] #progress_metric: host=algo-1, completed 25.0 % of epochs[0m
    [34m[02/25/2022 07:10:58 INFO 139844299908736] #quality_metric: host=algo-1, epoch=4, train loss <loss>=0.11159172307022593[0m
    [34m[02/25/2022 07:10:58 INFO 139844299908736] best epoch loss so far[0m
    [34m[02/25/2022 07:10:58 INFO 139844299908736] Saved checkpoint to "/opt/ml/model/state_e02e5c1f-b9e5-4026-b29b-1bbae9063ce1-0000.params"[0m
    [34m#metrics {"StartTime": 1645773058.4578927, "EndTime": 1645773058.5100687, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"state.serialize.time": {"sum": 51.676273345947266, "count": 1, "min": 51.676273345947266, "max": 51.676273345947266}}}[0m
    [34m[02/25/2022 07:10:58 INFO 139844299908736] Epoch[5] Batch[0] avg_epoch_loss=0.426447[0m
    [34m[02/25/2022 07:10:58 INFO 139844299908736] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=0.426447331905365[0m
    
    2022-02-25 07:10:54 Training - Training image download completed. Training in progress.[34m[02/25/2022 07:10:59 INFO 139844299908736] Epoch[5] Batch[5] avg_epoch_loss=0.025032[0m
    [34m[02/25/2022 07:10:59 INFO 139844299908736] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=0.025031663477420807[0m
    [34m[02/25/2022 07:10:59 INFO 139844299908736] Epoch[5] Batch [5]#011Speed: 219.55 samples/sec#011loss=0.025032[0m
    [34m[02/25/2022 07:11:00 INFO 139844299908736] processed a total of 316 examples[0m
    [34m#metrics {"StartTime": 1645773058.510144, "EndTime": 1645773060.0937283, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1583.5204124450684, "count": 1, "min": 1583.5204124450684, "max": 1583.5204124450684}}}[0m
    [34m[02/25/2022 07:11:00 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=199.5422698677336 records/second[0m
    [34m[02/25/2022 07:11:00 INFO 139844299908736] #progress_metric: host=algo-1, completed 30.0 % of epochs[0m
    [34m[02/25/2022 07:11:00 INFO 139844299908736] #quality_metric: host=algo-1, epoch=5, train loss <loss>=0.0134497482329607[0m
    [34m[02/25/2022 07:11:00 INFO 139844299908736] best epoch loss so far[0m
    [34m[02/25/2022 07:11:00 INFO 139844299908736] Saved checkpoint to "/opt/ml/model/state_188265ec-8ff4-4514-b946-da50c325b89a-0000.params"[0m
    [34m#metrics {"StartTime": 1645773060.0937996, "EndTime": 1645773060.1340184, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"state.serialize.time": {"sum": 39.67475891113281, "count": 1, "min": 39.67475891113281, "max": 39.67475891113281}}}[0m
    [34m[02/25/2022 07:11:00 INFO 139844299908736] Epoch[6] Batch[0] avg_epoch_loss=-0.460796[0m
    [34m[02/25/2022 07:11:00 INFO 139844299908736] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=-0.46079567074775696[0m
    [34m[02/25/2022 07:11:01 INFO 139844299908736] Epoch[6] Batch[5] avg_epoch_loss=-0.169595[0m
    [34m[02/25/2022 07:11:01 INFO 139844299908736] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=-0.1695948593939344[0m
    [34m[02/25/2022 07:11:01 INFO 139844299908736] Epoch[6] Batch [5]#011Speed: 215.32 samples/sec#011loss=-0.169595[0m
    [34m[02/25/2022 07:11:01 INFO 139844299908736] Epoch[6] Batch[10] avg_epoch_loss=-0.044823[0m
    [34m[02/25/2022 07:11:01 INFO 139844299908736] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=0.10490310788154603[0m
    [34m[02/25/2022 07:11:01 INFO 139844299908736] Epoch[6] Batch [10]#011Speed: 202.09 samples/sec#011loss=0.104903[0m
    [34m[02/25/2022 07:11:01 INFO 139844299908736] processed a total of 331 examples[0m
    [34m#metrics {"StartTime": 1645773060.1341069, "EndTime": 1645773061.9101877, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1776.0207653045654, "count": 1, "min": 1776.0207653045654, "max": 1776.0207653045654}}}[0m
    [34m[02/25/2022 07:11:01 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=186.3614625254041 records/second[0m
    [34m[02/25/2022 07:11:01 INFO 139844299908736] #progress_metric: host=algo-1, completed 35.0 % of epochs[0m
    [34m[02/25/2022 07:11:01 INFO 139844299908736] #quality_metric: host=algo-1, epoch=6, train loss <loss>=-0.04482305608689785[0m
    [34m[02/25/2022 07:11:01 INFO 139844299908736] best epoch loss so far[0m
    [34m[02/25/2022 07:11:01 INFO 139844299908736] Saved checkpoint to "/opt/ml/model/state_8087c1fd-3612-440c-ae4a-ed5947fa65cf-0000.params"[0m
    [34m#metrics {"StartTime": 1645773061.910254, "EndTime": 1645773061.966592, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"state.serialize.time": {"sum": 55.9844970703125, "count": 1, "min": 55.9844970703125, "max": 55.9844970703125}}}[0m
    [34m[02/25/2022 07:11:02 INFO 139844299908736] Epoch[7] Batch[0] avg_epoch_loss=-0.192869[0m
    [34m[02/25/2022 07:11:02 INFO 139844299908736] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=-0.1928691864013672[0m
    [34m[02/25/2022 07:11:03 INFO 139844299908736] Epoch[7] Batch[5] avg_epoch_loss=-0.089773[0m
    [34m[02/25/2022 07:11:03 INFO 139844299908736] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=-0.08977306758364041[0m
    [34m[02/25/2022 07:11:03 INFO 139844299908736] Epoch[7] Batch [5]#011Speed: 204.97 samples/sec#011loss=-0.089773[0m
    [34m[02/25/2022 07:11:03 INFO 139844299908736] processed a total of 312 examples[0m
    [34m#metrics {"StartTime": 1645773061.9666755, "EndTime": 1645773063.6649027, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1698.1444358825684, "count": 1, "min": 1698.1444358825684, "max": 1698.1444358825684}}}[0m
    [34m[02/25/2022 07:11:03 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=183.70467833712408 records/second[0m
    [34m[02/25/2022 07:11:03 INFO 139844299908736] #progress_metric: host=algo-1, completed 40.0 % of epochs[0m
    [34m[02/25/2022 07:11:03 INFO 139844299908736] #quality_metric: host=algo-1, epoch=7, train loss <loss>=-0.02890467904508114[0m
    [34m[02/25/2022 07:11:03 INFO 139844299908736] loss did not improve[0m
    [34m[02/25/2022 07:11:03 INFO 139844299908736] Epoch[8] Batch[0] avg_epoch_loss=-0.189289[0m
    [34m[02/25/2022 07:11:03 INFO 139844299908736] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=-0.1892886906862259[0m
    [34m[02/25/2022 07:11:04 INFO 139844299908736] Epoch[8] Batch[5] avg_epoch_loss=-0.067901[0m
    [34m[02/25/2022 07:11:04 INFO 139844299908736] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=-0.06790078400323789[0m
    [34m[02/25/2022 07:11:04 INFO 139844299908736] Epoch[8] Batch [5]#011Speed: 204.35 samples/sec#011loss=-0.067901[0m
    [34m[02/25/2022 07:11:05 INFO 139844299908736] Epoch[8] Batch[10] avg_epoch_loss=-0.044435[0m
    [34m[02/25/2022 07:11:05 INFO 139844299908736] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=-0.016275575011968614[0m
    [34m[02/25/2022 07:11:05 INFO 139844299908736] Epoch[8] Batch [10]#011Speed: 193.21 samples/sec#011loss=-0.016276[0m
    [34m[02/25/2022 07:11:05 INFO 139844299908736] processed a total of 323 examples[0m
    [34m#metrics {"StartTime": 1645773063.6650822, "EndTime": 1645773065.5712554, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1905.494213104248, "count": 1, "min": 1905.494213104248, "max": 1905.494213104248}}}[0m
    [34m[02/25/2022 07:11:05 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=169.50119816646063 records/second[0m
    [34m[02/25/2022 07:11:05 INFO 139844299908736] #progress_metric: host=algo-1, completed 45.0 % of epochs[0m
    [34m[02/25/2022 07:11:05 INFO 139844299908736] #quality_metric: host=algo-1, epoch=8, train loss <loss>=-0.0444347799162973[0m
    [34m[02/25/2022 07:11:05 INFO 139844299908736] loss did not improve[0m
    [34m[02/25/2022 07:11:05 INFO 139844299908736] Epoch[9] Batch[0] avg_epoch_loss=0.301513[0m
    [34m[02/25/2022 07:11:05 INFO 139844299908736] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=0.30151283740997314[0m
    [34m[02/25/2022 07:11:06 INFO 139844299908736] Epoch[9] Batch[5] avg_epoch_loss=0.102831[0m
    [34m[02/25/2022 07:11:06 INFO 139844299908736] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=0.10283090981344382[0m
    [34m[02/25/2022 07:11:06 INFO 139844299908736] Epoch[9] Batch [5]#011Speed: 214.19 samples/sec#011loss=0.102831[0m
    [34m[02/25/2022 07:11:07 INFO 139844299908736] processed a total of 290 examples[0m
    [34m#metrics {"StartTime": 1645773065.5713198, "EndTime": 1645773067.1802373, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1608.574628829956, "count": 1, "min": 1608.574628829956, "max": 1608.574628829956}}}[0m
    [34m[02/25/2022 07:11:07 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=180.27196973009183 records/second[0m
    [34m[02/25/2022 07:11:07 INFO 139844299908736] #progress_metric: host=algo-1, completed 50.0 % of epochs[0m
    [34m[02/25/2022 07:11:07 INFO 139844299908736] #quality_metric: host=algo-1, epoch=9, train loss <loss>=-0.1154014240950346[0m
    [34m[02/25/2022 07:11:07 INFO 139844299908736] best epoch loss so far[0m
    [34m[02/25/2022 07:11:07 INFO 139844299908736] Saved checkpoint to "/opt/ml/model/state_4968fff7-8073-427c-a9b3-4a2409a5f801-0000.params"[0m
    [34m#metrics {"StartTime": 1645773067.1803105, "EndTime": 1645773067.2338054, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"state.serialize.time": {"sum": 53.102731704711914, "count": 1, "min": 53.102731704711914, "max": 53.102731704711914}}}[0m
    [34m[02/25/2022 07:11:07 INFO 139844299908736] Epoch[10] Batch[0] avg_epoch_loss=-0.146316[0m
    [34m[02/25/2022 07:11:07 INFO 139844299908736] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=-0.14631634950637817[0m
    [34m[02/25/2022 07:11:08 INFO 139844299908736] Epoch[10] Batch[5] avg_epoch_loss=-0.224772[0m
    [34m[02/25/2022 07:11:08 INFO 139844299908736] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=-0.22477184484402338[0m
    [34m[02/25/2022 07:11:08 INFO 139844299908736] Epoch[10] Batch [5]#011Speed: 207.30 samples/sec#011loss=-0.224772[0m
    [34m[02/25/2022 07:11:09 INFO 139844299908736] Epoch[10] Batch[10] avg_epoch_loss=-0.155849[0m
    [34m[02/25/2022 07:11:09 INFO 139844299908736] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=-0.07314214501529932[0m
    [34m[02/25/2022 07:11:09 INFO 139844299908736] Epoch[10] Batch [10]#011Speed: 200.29 samples/sec#011loss=-0.073142[0m
    [34m[02/25/2022 07:11:09 INFO 139844299908736] processed a total of 364 examples[0m
    [34m#metrics {"StartTime": 1645773067.2338717, "EndTime": 1645773069.2492886, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 2015.3522491455078, "count": 1, "min": 2015.3522491455078, "max": 2015.3522491455078}}}[0m
    [34m[02/25/2022 07:11:09 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=180.6041226042454 records/second[0m
    [34m[02/25/2022 07:11:09 INFO 139844299908736] #progress_metric: host=algo-1, completed 55.0 % of epochs[0m
    [34m[02/25/2022 07:11:09 INFO 139844299908736] #quality_metric: host=algo-1, epoch=10, train loss <loss>=-0.12133395959002276[0m
    [34m[02/25/2022 07:11:09 INFO 139844299908736] best epoch loss so far[0m
    [34m[02/25/2022 07:11:09 INFO 139844299908736] Saved checkpoint to "/opt/ml/model/state_e01dd4a6-8052-4ab6-ab44-e319b0b991d9-0000.params"[0m
    [34m#metrics {"StartTime": 1645773069.2493603, "EndTime": 1645773069.3032057, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"state.serialize.time": {"sum": 53.468942642211914, "count": 1, "min": 53.468942642211914, "max": 53.468942642211914}}}[0m
    [34m[02/25/2022 07:11:09 INFO 139844299908736] Epoch[11] Batch[0] avg_epoch_loss=-0.274687[0m
    [34m[02/25/2022 07:11:09 INFO 139844299908736] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=-0.2746865749359131[0m
    [34m[02/25/2022 07:11:10 INFO 139844299908736] Epoch[11] Batch[5] avg_epoch_loss=-0.087261[0m
    [34m[02/25/2022 07:11:10 INFO 139844299908736] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=-0.08726102610429128[0m
    [34m[02/25/2022 07:11:10 INFO 139844299908736] Epoch[11] Batch [5]#011Speed: 208.79 samples/sec#011loss=-0.087261[0m
    [34m[02/25/2022 07:11:10 INFO 139844299908736] processed a total of 290 examples[0m
    [34m#metrics {"StartTime": 1645773069.3032696, "EndTime": 1645773070.9483445, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1645.0095176696777, "count": 1, "min": 1645.0095176696777, "max": 1645.0095176696777}}}[0m
    [34m[02/25/2022 07:11:10 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=176.27907192446284 records/second[0m
    [34m[02/25/2022 07:11:10 INFO 139844299908736] #progress_metric: host=algo-1, completed 60.0 % of epochs[0m
    [34m[02/25/2022 07:11:10 INFO 139844299908736] #quality_metric: host=algo-1, epoch=11, train loss <loss>=0.06326946690678596[0m
    [34m[02/25/2022 07:11:10 INFO 139844299908736] loss did not improve[0m
    [34m[02/25/2022 07:11:11 INFO 139844299908736] Epoch[12] Batch[0] avg_epoch_loss=-0.704607[0m
    [34m[02/25/2022 07:11:11 INFO 139844299908736] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=-0.7046068906784058[0m
    [34m[02/25/2022 07:11:11 INFO 139844299908736] Epoch[12] Batch[5] avg_epoch_loss=-0.140154[0m
    [34m[02/25/2022 07:11:11 INFO 139844299908736] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=-0.14015381410717964[0m
    [34m[02/25/2022 07:11:11 INFO 139844299908736] Epoch[12] Batch [5]#011Speed: 214.89 samples/sec#011loss=-0.140154[0m
    [34m[02/25/2022 07:11:12 INFO 139844299908736] processed a total of 288 examples[0m
    [34m#metrics {"StartTime": 1645773070.9484198, "EndTime": 1645773072.4431815, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1494.1904544830322, "count": 1, "min": 1494.1904544830322, "max": 1494.1904544830322}}}[0m
    [34m[02/25/2022 07:11:12 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=192.7328586861848 records/second[0m
    [34m[02/25/2022 07:11:12 INFO 139844299908736] #progress_metric: host=algo-1, completed 65.0 % of epochs[0m
    [34m[02/25/2022 07:11:12 INFO 139844299908736] #quality_metric: host=algo-1, epoch=12, train loss <loss>=-0.09334910164276759[0m
    [34m[02/25/2022 07:11:12 INFO 139844299908736] loss did not improve[0m
    [34m[02/25/2022 07:11:12 INFO 139844299908736] Epoch[13] Batch[0] avg_epoch_loss=0.061250[0m
    [34m[02/25/2022 07:11:12 INFO 139844299908736] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=0.06124971807003021[0m
    [34m[02/25/2022 07:11:13 INFO 139844299908736] Epoch[13] Batch[5] avg_epoch_loss=-0.011601[0m
    [34m[02/25/2022 07:11:13 INFO 139844299908736] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=-0.011601145068804422[0m
    [34m[02/25/2022 07:11:13 INFO 139844299908736] Epoch[13] Batch [5]#011Speed: 211.75 samples/sec#011loss=-0.011601[0m
    [34m[02/25/2022 07:11:14 INFO 139844299908736] processed a total of 317 examples[0m
    [34m#metrics {"StartTime": 1645773072.443255, "EndTime": 1645773074.092655, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1649.0328311920166, "count": 1, "min": 1649.0328311920166, "max": 1649.0328311920166}}}[0m
    [34m[02/25/2022 07:11:14 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=192.2216086281498 records/second[0m
    [34m[02/25/2022 07:11:14 INFO 139844299908736] #progress_metric: host=algo-1, completed 70.0 % of epochs[0m
    [34m[02/25/2022 07:11:14 INFO 139844299908736] #quality_metric: host=algo-1, epoch=13, train loss <loss>=8.765235543251038e-05[0m
    [34m[02/25/2022 07:11:14 INFO 139844299908736] loss did not improve[0m
    [34m[02/25/2022 07:11:14 INFO 139844299908736] Epoch[14] Batch[0] avg_epoch_loss=0.098831[0m
    [34m[02/25/2022 07:11:14 INFO 139844299908736] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=0.09883131831884384[0m
    [34m[02/25/2022 07:11:15 INFO 139844299908736] Epoch[14] Batch[5] avg_epoch_loss=-0.206959[0m
    [34m[02/25/2022 07:11:15 INFO 139844299908736] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=-0.20695899737377962[0m
    [34m[02/25/2022 07:11:15 INFO 139844299908736] Epoch[14] Batch [5]#011Speed: 229.81 samples/sec#011loss=-0.206959[0m
    [34m[02/25/2022 07:11:15 INFO 139844299908736] processed a total of 314 examples[0m
    [34m#metrics {"StartTime": 1645773074.0927265, "EndTime": 1645773075.6513138, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1558.1886768341064, "count": 1, "min": 1558.1886768341064, "max": 1558.1886768341064}}}[0m
    [34m[02/25/2022 07:11:15 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=201.50292888155985 records/second[0m
    [34m[02/25/2022 07:11:15 INFO 139844299908736] #progress_metric: host=algo-1, completed 75.0 % of epochs[0m
    [34m[02/25/2022 07:11:15 INFO 139844299908736] #quality_metric: host=algo-1, epoch=14, train loss <loss>=-0.1284581024199724[0m
    [34m[02/25/2022 07:11:15 INFO 139844299908736] best epoch loss so far[0m
    [34m[02/25/2022 07:11:15 INFO 139844299908736] Saved checkpoint to "/opt/ml/model/state_8e361d8b-ee28-4d05-bc00-1218aa47fbd2-0000.params"[0m
    [34m#metrics {"StartTime": 1645773075.6513836, "EndTime": 1645773075.69764, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"state.serialize.time": {"sum": 45.87602615356445, "count": 1, "min": 45.87602615356445, "max": 45.87602615356445}}}[0m
    [34m[02/25/2022 07:11:15 INFO 139844299908736] Epoch[15] Batch[0] avg_epoch_loss=-0.110106[0m
    [34m[02/25/2022 07:11:15 INFO 139844299908736] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=-0.1101064682006836[0m
    [34m[02/25/2022 07:11:16 INFO 139844299908736] Epoch[15] Batch[5] avg_epoch_loss=-0.255051[0m
    [34m[02/25/2022 07:11:16 INFO 139844299908736] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=-0.255051222940286[0m
    [34m[02/25/2022 07:11:16 INFO 139844299908736] Epoch[15] Batch [5]#011Speed: 222.75 samples/sec#011loss=-0.255051[0m
    [34m[02/25/2022 07:11:17 INFO 139844299908736] processed a total of 308 examples[0m
    [34m#metrics {"StartTime": 1645773075.69771, "EndTime": 1645773077.2915208, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1593.7366485595703, "count": 1, "min": 1593.7366485595703, "max": 1593.7366485595703}}}[0m
    [34m[02/25/2022 07:11:17 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=193.2390026838301 records/second[0m
    [34m[02/25/2022 07:11:17 INFO 139844299908736] #progress_metric: host=algo-1, completed 80.0 % of epochs[0m
    [34m[02/25/2022 07:11:17 INFO 139844299908736] #quality_metric: host=algo-1, epoch=15, train loss <loss>=-0.12527945041656494[0m
    [34m[02/25/2022 07:11:17 INFO 139844299908736] loss did not improve[0m
    [34m[02/25/2022 07:11:17 INFO 139844299908736] Epoch[16] Batch[0] avg_epoch_loss=-0.138310[0m
    [34m[02/25/2022 07:11:17 INFO 139844299908736] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=-0.138309508562088[0m
    [34m[02/25/2022 07:11:18 INFO 139844299908736] Epoch[16] Batch[5] avg_epoch_loss=-0.174127[0m
    [34m[02/25/2022 07:11:18 INFO 139844299908736] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=-0.1741273015116652[0m
    [34m[02/25/2022 07:11:18 INFO 139844299908736] Epoch[16] Batch [5]#011Speed: 206.95 samples/sec#011loss=-0.174127[0m
    [34m[02/25/2022 07:11:19 INFO 139844299908736] Epoch[16] Batch[10] avg_epoch_loss=-0.035435[0m
    [34m[02/25/2022 07:11:19 INFO 139844299908736] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=0.13099481612443925[0m
    [34m[02/25/2022 07:11:19 INFO 139844299908736] Epoch[16] Batch [10]#011Speed: 206.90 samples/sec#011loss=0.130995[0m
    [34m[02/25/2022 07:11:19 INFO 139844299908736] processed a total of 323 examples[0m
    [34m#metrics {"StartTime": 1645773077.2916205, "EndTime": 1645773079.1012423, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1809.110164642334, "count": 1, "min": 1809.110164642334, "max": 1809.110164642334}}}[0m
    [34m[02/25/2022 07:11:19 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=178.53093137953854 records/second[0m
    [34m[02/25/2022 07:11:19 INFO 139844299908736] #progress_metric: host=algo-1, completed 85.0 % of epochs[0m
    [34m[02/25/2022 07:11:19 INFO 139844299908736] #quality_metric: host=algo-1, epoch=16, train loss <loss>=-0.03543542985889045[0m
    [34m[02/25/2022 07:11:19 INFO 139844299908736] loss did not improve[0m
    [34m[02/25/2022 07:11:19 INFO 139844299908736] Epoch[17] Batch[0] avg_epoch_loss=-0.110838[0m
    [34m[02/25/2022 07:11:19 INFO 139844299908736] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=-0.11083836853504181[0m
    [34m[02/25/2022 07:11:20 INFO 139844299908736] Epoch[17] Batch[5] avg_epoch_loss=-0.195874[0m
    [34m[02/25/2022 07:11:20 INFO 139844299908736] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=-0.19587354610363641[0m
    [34m[02/25/2022 07:11:20 INFO 139844299908736] Epoch[17] Batch [5]#011Speed: 207.66 samples/sec#011loss=-0.195874[0m
    [34m[02/25/2022 07:11:20 INFO 139844299908736] processed a total of 304 examples[0m
    [34m#metrics {"StartTime": 1645773079.1013107, "EndTime": 1645773080.7758982, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1674.2427349090576, "count": 1, "min": 1674.2427349090576, "max": 1674.2427349090576}}}[0m
    [34m[02/25/2022 07:11:20 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=181.56200963978463 records/second[0m
    [34m[02/25/2022 07:11:20 INFO 139844299908736] #progress_metric: host=algo-1, completed 90.0 % of epochs[0m
    [34m[02/25/2022 07:11:20 INFO 139844299908736] #quality_metric: host=algo-1, epoch=17, train loss <loss>=-0.11305377744138241[0m
    [34m[02/25/2022 07:11:20 INFO 139844299908736] loss did not improve[0m
    [34m[02/25/2022 07:11:21 INFO 139844299908736] Epoch[18] Batch[0] avg_epoch_loss=0.031934[0m
    [34m[02/25/2022 07:11:21 INFO 139844299908736] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=0.0319337323307991[0m
    [34m[02/25/2022 07:11:21 INFO 139844299908736] Epoch[18] Batch[5] avg_epoch_loss=-0.042119[0m
    [34m[02/25/2022 07:11:21 INFO 139844299908736] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=-0.04211917736877998[0m
    [34m[02/25/2022 07:11:21 INFO 139844299908736] Epoch[18] Batch [5]#011Speed: 207.19 samples/sec#011loss=-0.042119[0m
    [34m[02/25/2022 07:11:22 INFO 139844299908736] Epoch[18] Batch[10] avg_epoch_loss=-0.046443[0m
    [34m[02/25/2022 07:11:22 INFO 139844299908736] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=-0.051630548760294916[0m
    [34m[02/25/2022 07:11:22 INFO 139844299908736] Epoch[18] Batch [10]#011Speed: 199.13 samples/sec#011loss=-0.051631[0m
    [34m[02/25/2022 07:11:22 INFO 139844299908736] processed a total of 336 examples[0m
    [34m#metrics {"StartTime": 1645773080.7759795, "EndTime": 1645773082.6062577, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1829.8454284667969, "count": 1, "min": 1829.8454284667969, "max": 1829.8454284667969}}}[0m
    [34m[02/25/2022 07:11:22 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=183.61223445635432 records/second[0m
    [34m[02/25/2022 07:11:22 INFO 139844299908736] #progress_metric: host=algo-1, completed 95.0 % of epochs[0m
    [34m[02/25/2022 07:11:22 INFO 139844299908736] #quality_metric: host=algo-1, epoch=18, train loss <loss>=-0.046442528001286766[0m
    [34m[02/25/2022 07:11:22 INFO 139844299908736] loss did not improve[0m
    [34m[02/25/2022 07:11:22 INFO 139844299908736] Epoch[19] Batch[0] avg_epoch_loss=-0.250460[0m
    [34m[02/25/2022 07:11:22 INFO 139844299908736] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=-0.25045979022979736[0m
    [34m[02/25/2022 07:11:23 INFO 139844299908736] Epoch[19] Batch[5] avg_epoch_loss=-0.106722[0m
    [34m[02/25/2022 07:11:23 INFO 139844299908736] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=-0.10672195442020893[0m
    [34m[02/25/2022 07:11:23 INFO 139844299908736] Epoch[19] Batch [5]#011Speed: 209.54 samples/sec#011loss=-0.106722[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] processed a total of 318 examples[0m
    [34m#metrics {"StartTime": 1645773082.6063247, "EndTime": 1645773084.2362995, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"update.time": {"sum": 1629.6374797821045, "count": 1, "min": 1629.6374797821045, "max": 1629.6374797821045}}}[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] #throughput_metric: host=algo-1, train throughput=195.1219210763308 records/second[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] #progress_metric: host=algo-1, completed 100.0 % of epochs[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] #quality_metric: host=algo-1, epoch=19, train loss <loss>=-0.08941519521176815[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] loss did not improve[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] Final loss: -0.1284581024199724 (occurred at epoch 14)[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] #quality_metric: host=algo-1, train final_loss <loss>=-0.1284581024199724[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] Worker algo-1 finished training.[0m
    [34m[02/25/2022 07:11:24 WARNING 139844299908736] wait_for_all_workers will not sync workers since the kv store is not running distributed[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] All workers finished. Serializing model for prediction.[0m
    [34m#metrics {"StartTime": 1645773084.2363768, "EndTime": 1645773084.7342925, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"get_graph.time": {"sum": 496.9661235809326, "count": 1, "min": 496.9661235809326, "max": 496.9661235809326}}}[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] Number of GPUs being used: 0[0m
    [34m#metrics {"StartTime": 1645773084.7343607, "EndTime": 1645773084.9241216, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"finalize.time": {"sum": 686.8278980255127, "count": 1, "min": 686.8278980255127, "max": 686.8278980255127}}}[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] Serializing to /opt/ml/model/model_algo-1[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] Saved checkpoint to "/opt/ml/model/model_algo-1-0000.params"[0m
    [34m#metrics {"StartTime": 1645773084.9241924, "EndTime": 1645773084.9495714, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"model.serialize.time": {"sum": 25.338172912597656, "count": 1, "min": 25.338172912597656, "max": 25.338172912597656}}}[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] Successfully serialized the model for prediction.[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] #memory_usage::<batchbuffer> = 3.5982131958007812 mb[0m
    [34m[02/25/2022 07:11:24 INFO 139844299908736] Evaluating model accuracy on testset using 100 samples[0m
    [34m#metrics {"StartTime": 1645773084.9496238, "EndTime": 1645773084.9510815, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"model.bind.time": {"sum": 0.04029273986816406, "count": 1, "min": 0.04029273986816406, "max": 0.04029273986816406}}}[0m
    [34m#metrics {"StartTime": 1645773084.9511366, "EndTime": 1645773096.8902702, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"model.score.time": {"sum": 11939.229011535645, "count": 1, "min": 11939.229011535645, "max": 11939.229011535645}}}[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #test_score (algo-1, RMSE): 0.3283896350838211[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #test_score (algo-1, mean_absolute_QuantileLoss): 1602.989599926656[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #test_score (algo-1, mean_wQuantileLoss): 0.03412824351371161[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #test_score (algo-1, wQuantileLoss[0.1]): 0.020079625469613012[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #test_score (algo-1, wQuantileLoss[0.2]): 0.03135376584196203[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #test_score (algo-1, wQuantileLoss[0.3]): 0.038603218402800836[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #test_score (algo-1, wQuantileLoss[0.4]): 0.04269301202355334[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #test_score (algo-1, wQuantileLoss[0.5]): 0.04390008467244175[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #test_score (algo-1, wQuantileLoss[0.6]): 0.042386337979702204[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #test_score (algo-1, wQuantileLoss[0.7]): 0.03810158180564604[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #test_score (algo-1, wQuantileLoss[0.8]): 0.030714157750925713[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #test_score (algo-1, wQuantileLoss[0.9]): 0.019322407676759613[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #quality_metric: host=algo-1, test RMSE <loss>=0.3283896350838211[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.03412824351371161[0m
    [34m#metrics {"StartTime": 1645773096.8903544, "EndTime": 1645773096.939641, "Dimensions": {"Algorithm": "AWS/DeepAR", "Host": "algo-1", "Operation": "training"}, "Metrics": {"setuptime": {"sum": 5.819797515869141, "count": 1, "min": 5.819797515869141, "max": 5.819797515869141}, "totaltime": {"sum": 48216.641664505005, "count": 1, "min": 48216.641664505005, "max": 48216.641664505005}}}[0m
    [34m[02/25/2022 07:11:36 INFO 139844299908736 integration.py:592] worker closed[0m
    
    2022-02-25 07:11:56 Uploading - Uploading generated training model
    2022-02-25 07:11:56 Completed - Training job completed
    Training seconds: 147
    Billable seconds: 147


### Create endpoint and predictor

Now that we have trained a model, we can use it to perform predictions by deploying it to an endpoint.

**Note:** remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.


```python
job_name = estimator.latest_training_job.name

# API ì™€ Endpointì˜ ì°¨ì´ í•œ ì¤„ ì •ë¦¬

'''

ì§€í•˜ì²  ìµœë‹¨ ê±°ë¦¬ ê²½ë¡œë¥¼ ì œê³µí•˜ëŠ” ì›¹ ì„œë¹„ìŠ¤ê°€ ìˆë‹¤ê³  í•˜ì.

ì´ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ëŠ” ì‚¬ìš©ìëŠ” ì¶œë°œì—­ê³¼ ë„ì°©ì—­ì„ ì„¤ì •í•˜ê³  ìµœë‹¨ ê²½ë¡œë¥¼ ì°¾ëŠ” ë²„íŠ¼ì„ ëˆ„ë¥¸ë‹¤.


ì´ ë•Œ ìµœë‹¨ ê±°ë¦¬ ê²½ë¡œë¥¼ êµ¬í•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ê¸° ìœ„í•œ ìš”ì²­ì´ í–¥í•˜ëŠ” URIê°€, ì—”ë“œí¬ì¸íŠ¸ì´ë‹¤.
'''

endpoint_name = sagemaker_session.endpoint_from_job(
    job_name=job_name,
    initial_instance_count=1,
    instance_type="ml.m5.2xlarge",
    image_uri=image_uri,
    role=role,
)
```

    ------!

To query the endpoint and perform predictions, we can define the following utility class: this allows making requests using `pandas.Series` objects rather than raw JSON strings.

- predictor 

Make real-time predictions against SageMaker endpoints with Python objects




```python
class DeepARPredictor(sagemaker.predictor.RealTimePredictor):
    def set_prediction_parameters(self, freq, prediction_length):
        """Set the time frequency and prediction length parameters. This method **must** be called
        before being able to use `predict`.

        Parameters:
        freq -- string indicating the time frequency
        prediction_length -- integer, number of predicted time points

        Return value: none.
        """
        self.freq = freq
        self.prediction_length = prediction_length

    def predict(
        self,
        ts,  # serise ê°ì²´
        cat=None, # default ê°’ ì„¤ì • 
        encoding="utf-8",
        num_samples=100, # ì˜ˆì¸¡ ìƒ˜í”Œë§ ë°ì´í„° ìˆ˜ 100ê°œ ì„¤ì • 
        quantiles=["0.1", "0.5", "0.9"],  # ë¶„ìœ„ìˆ˜ íšŒê·€(QUANTILE REGRESSION) / ìš°ë¦¬ê°€ í”íˆ í•˜ëŠ” íšŒê·€ë¶„ì„ì€ ì£¼ì–´ì§„ ìƒí™©(ì˜ˆì¸¡ ë³€ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ), ê²°ê³¼ ë³€ìˆ˜ì˜ í‰ê· ì„ ì˜ˆì¸¡í•œë‹¤. ì´ì— ë°˜í•´ Quantile regressionì€ ì˜ˆì¸¡ ë³€ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê²°ê³¼ ë³€ìˆ˜ì˜ q-ë¶„ìœ„ìˆ˜ë¥¼ ì˜ˆì¸¡í•œë‹¤.
        content_type="application/json", # ìœ„ì—ì„œ ë°ì´í„°ë¥¼ json í˜•íƒœë¡œ ë°”ê¾¼ ê²ƒì„ ì ìš©í•¨
    ):
        """Requests the prediction of for the time series listed in `ts`, each with the (optional)
        corresponding category listed in `cat`.

        Parameters:
        ts -- list of `pandas.Series` objects, the time series to predict
        cat -- list of integers (default: None)
        encoding -- string, encoding to use for the request (default: "utf-8")
        num_samples -- integer, number of samples to compute at prediction time (default: 100)
        quantiles -- list of strings specifying the quantiles to compute (default: ["0.1", "0.5", "0.9"])

        Return value: list of `pandas.DataFrame` objects, each containing the predictions
        """
        prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts] # Timedeltaí•¨ìˆ˜ë¥¼ ì ìš©ì‹œì¼œ 1ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ ì‹œê°„ì„ ì •í•  ìˆ˜ ìˆê²Œ ì„¤ì •í•¨ 
        req = self.__encode_request(ts, cat, encoding, num_samples, quantiles) # encode í•œ ë°ì´í„°ë¥¼ request í•´ì¤Œ , ë°‘ì— __encode_request í•¨ìˆ˜ ì‹¤í–‰ì‹œì¼œì¤Œ
        # superí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ sagemaker.predictor.RealTimePredictor ë¶€ëª¨í´ë˜ìŠ¤ì—ì„œ í•¨ìˆ˜ ê°€ì ¸ì™€ ì‚¬ìš©í•¨
        res = super(DeepARPredictor, self).predict(req, initial_args={"ContentType": content_type})
        return self.__decode_response(res, prediction_times, encoding) # decode í•œ ë°ì´í„°ë¥¼ response í•´ì¤Œ, ë°‘ì— __decode_response í•¨ìˆ˜ ì‹¤í–‰ì‹œì¼œì¤Œ

    def __encode_request(self, ts, cat, encoding, num_samples, quantiles):
        instances = [series_to_obj(ts[k], cat[k] if cat else None) for k in range(len(ts))] # series ë°ì´í„°ë¥¼ json ê°ì²´ë¡œ ë³€í™˜ í•´ì£¼ëŠ” series_to_obj í•¨ìˆ˜ ì ìš© 
        configuration = {
            "num_samples": num_samples, # ìœ„ì—ì„œ ì„¤ì •í•œ num_samples=100, # ì˜ˆì¸¡ ìƒ˜í”Œë§ ë°ì´í„° ìˆ˜ 100ê°œ ì„¤ì • ì ìš© ì‹œì¼œì¤Œ 
            "output_types": ["quantiles"], # ìœ„ì—ì„œ ì„¤ì •í•œ quantiles=["0.1", "0.5", "0.9"] ì ìš©ì‹œí‚´ 
            "quantiles": quantiles,
        }
        http_request_data = {"instances": instances, "configuration": configuration} # instances, configurationë¥¼ dictionaryìœ¼ë¡œ ë§Œë“¤ì–´ì¤Œ 
        return json.dumps(http_request_data).encode(encoding) # ì‚¬ì „í˜•(dictí˜•)ì„ JSONë¬¸ìì—´ë¡œ ì •í˜•í™”í•˜ì—¬ ì¶œë ¥ : json.dumps()

    def __decode_response(self, response, prediction_times, encoding):
        response_data = json.loads(response.decode(encoding)) # JSONë¬¸ìì—´ì„ ì‚¬ì „í˜•ìœ¼ë¡œ ë³€í™˜ : json.loads()

        list_of_df = []
        for k in range(len(prediction_times)): # 1ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡í•  ì‹œê³„ì—´ ë°ì´í„° ì ìš©
            prediction_index = pd.date_range(
                start=prediction_times[k], freq=self.freq, periods=self.prediction_length   # ìœ„ì—ì„œ ì„¤ì •í•œ ë³€ìˆ˜ prediction_length = 48 ì‹œê°„ ì ìš©í•¨
            )
            list_of_df.append(
                pd.DataFrame(
                    data=response_data["predictions"][k]["quantiles"], index=prediction_index
                )
            )
        return list_of_df
```


```python
predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)  # predictor ë³€ìˆ˜ ì„¤ì •í•¨
predictor.set_prediction_parameters(freq, prediction_length)
```

    The class RealTimePredictor has been renamed in sagemaker>=2.
    See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.


### Make predictions and plot results

Now we can use the previously created `predictor` object. For simplicity, we will predict only the first few time series used for training, and compare the results with the actual data we kept in the test set.

- time_series_training[:15] ,  time_series[:15] ìœ¼ë¡œ ë³€ê²½í•˜ë©´ ê·¸ë˜í”„ì˜ xì¶• ê°’, yì¶• ê°’ì´ ì•ˆë³´ì„ , ë”°ë¼ì„œ 5ìœ¼ë¡œ ì„¤ì •í•œ ê²ƒì´ ê°€ì¥ ì´ìƒì ì´ë‹¤.


```python
list_of_df = predictor.predict(time_series_training[:5], content_type="application/json") # í›ˆë ¨ë°ì´í„° ì ìš©ì‹œí‚´
actual_data = time_series[:5] 
```


```python
for k in range(len(list_of_df)): #  í›ˆë ¨ë°ì´í„° ë²”ìœ„ë¡œ ì§€ì •í•´ì¤Œ
    plt.figure(figsize=(12, 6))
    actual_data[k][-prediction_length - context_length :].plot(label="target") # yì¶•ì„ prediction_length + context_length ë²”ìœ„ë¡œ ì§€ì •í•´ì¤Œ
    p10 = list_of_df[k]["0.1"] # ìœ„ì—ì„œ ì„¤ì •í•œ quantiles 0.1ìœ¼ë¡œ ì„¤ì •í•´ì¤Œ
    p90 = list_of_df[k]["0.9"] # ìœ„ì—ì„œ ì„¤ì •í•œ quantiles 0.9ìœ¼ë¡œ ì„¤ì •í•´ì¤Œ
    '''
    matplotlib.pyplot.fill_between()
    The matplotlib.pyplot.fill_between() is used to fill area between two horizontal curves
    '''
    plt.fill_between(p10.index, p10, p90, color="y", alpha=0.5, label="80% confidence interval")
    list_of_df[k]["0.5"].plot(label="prediction median")
    plt.legend()
    plt.show()
```

![output_60_0]({{ site.gdrive_url_prefix }}12L1mPzl3kPtALe7mVBF5cGdUE3Xyu99l)



![output_60_1]({{ site.gdrive_url_prefix }}1MoFOKX_f2Vcuh87-uu3dBryh99JldFOZ)


![output_60_2]({{ site.gdrive_url_prefix }}1P2GpzemrmcH-XJBUHfmrupeandt8B92N)



![output_60_3]({{ site.gdrive_url_prefix }}1kxYLsmzwrRoSk6L_3s0uOEZqFBeuo7Df)


![output_60_4]({{ site.gdrive_url_prefix }}16eIdWzauI2nQMlcpHHigmc6wb96uIyyy)



### Delete endpoint


```python
sagemaker_session.delete_endpoint(endpoint_name) # ë§ˆì§€ë§‰ì— endpoint ì‚­ì œí•´ì¤˜ì•¼í•¨
```

